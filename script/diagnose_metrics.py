#!/usr/bin/env python
# script/diagnose_metrics.py - è¯Šæ–­å®šé‡æŒ‡æ ‡è®¡ç®—é—®é¢˜

import sys
import os
import torch
import numpy as np
import matplotlib.pyplot as plt

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from model.asc_net import ASCNet_v3_5param
from utils import config
from utils.dataset import MSTAR_ASC_5CH_Dataset, read_sar_complex_tensor
from utils.reconstruction import reconstruct_sar_image, pixel_to_model
from script.quantitative_analysis import QuantitativeAnalyzer


class MetricsDiagnosis:
    """å®šé‡æŒ‡æ ‡è¯Šæ–­å·¥å…·"""

    def __init__(self):
        self.analyzer = QuantitativeAnalyzer()

    def diagnose_single_sample(self, sar_path, model):
        """æ·±åº¦è¯Šæ–­å•ä¸ªæ ·æœ¬"""
        base_name = os.path.basename(sar_path).replace(".raw", "")
        print(f"\nğŸ”¬ æ·±åº¦è¯Šæ–­æ ·æœ¬: {base_name}")
        print("=" * 60)

        # 1. åŠ è½½åŸå§‹SARå›¾åƒ
        original_tensor = read_sar_complex_tensor(sar_path, config.IMG_HEIGHT, config.IMG_WIDTH)
        if original_tensor is None:
            print("âŒ æ— æ³•åŠ è½½åŸå§‹SARå›¾åƒ")
            return

        original_complex = original_tensor.numpy()
        original_magnitude = np.abs(original_complex)

        print(f"ğŸ“Š åŸå§‹å›¾åƒåˆ†æ:")
        print(f"  å½¢çŠ¶: {original_complex.shape}")
        print(f"  å¹…åº¦èŒƒå›´: {original_magnitude.min():.6f} ~ {original_magnitude.max():.6f}")
        print(f"  å¹³å‡å¹…åº¦: {original_magnitude.mean():.6f}")
        print(f"  RMS: {np.sqrt(np.mean(original_magnitude**2)):.6f}")

        # 2. æ¨¡å‹é¢„æµ‹
        with torch.no_grad():
            input_tensor = original_tensor.unsqueeze(0).to(config.DEVICE)
            predicted_maps = model(input_tensor).squeeze(0).cpu().numpy()

        print(f"\nğŸ¤– æ¨¡å‹é¢„æµ‹åˆ†æ:")
        for i, name in enumerate(["heatmap", "A_log1p", "alpha", "dx", "dy"]):
            pred_map = predicted_maps[i]
            print(f"  {name}: èŒƒå›´[{pred_map.min():.3f}, {pred_map.max():.3f}], å‡å€¼{pred_map.mean():.3f}")

        # 3. æå–æ•£å°„ä¸­å¿ƒ (Aggressiveæ¨¡å¼)
        predicted_scatterers = self.analyzer.extract_scatterers_aggressive(predicted_maps)
        print(f"\nğŸ¯ æ•£å°„ä¸­å¿ƒæå– (Aggressive):")
        print(f"  æ£€å‡ºæ•°é‡: {len(predicted_scatterers)}")

        if predicted_scatterers:
            amplitudes = [s["A"] for s in predicted_scatterers]
            print(f"  å¹…åº¦èŒƒå›´: {min(amplitudes):.4f} ~ {max(amplitudes):.4f}")
            print(f"  å¹³å‡å¹…åº¦: {np.mean(amplitudes):.4f}")

        # 4. é‡å»ºSARå›¾åƒ
        reconstructed_complex = reconstruct_sar_image(predicted_scatterers)
        reconstructed_magnitude = np.abs(reconstructed_complex)

        print(f"\nğŸ”§ é‡å»ºå›¾åƒåˆ†æ:")
        print(f"  å½¢çŠ¶: {reconstructed_complex.shape}")
        print(f"  å¹…åº¦èŒƒå›´: {reconstructed_magnitude.min():.6f} ~ {reconstructed_magnitude.max():.6f}")
        print(f"  å¹³å‡å¹…åº¦: {reconstructed_magnitude.mean():.6f}")
        print(f"  RMS: {np.sqrt(np.mean(reconstructed_magnitude**2)):.6f}")

        # 5. MSEè®¡ç®—è¯Šæ–­
        print(f"\nğŸ“ MSEè®¡ç®—è¯Šæ–­:")

        # æ–¹æ³•1ï¼šå½“å‰ä½¿ç”¨çš„ç›¸å¯¹MSE
        original_norm = np.linalg.norm(original_complex)
        diff_norm = np.linalg.norm(reconstructed_complex - original_complex)
        current_mse = diff_norm / original_norm
        print(f"  å½“å‰æ–¹æ³• (ç›¸å¯¹RMSE): {current_mse:.6f}")

        # æ–¹æ³•2ï¼šä¼ ç»ŸMSE
        traditional_mse = np.mean(np.abs(reconstructed_complex - original_complex) ** 2)
        print(f"  ä¼ ç»ŸMSE: {traditional_mse:.6f}")

        # æ–¹æ³•3ï¼šå½’ä¸€åŒ–MSE
        normalized_mse = traditional_mse / np.mean(np.abs(original_complex) ** 2)
        print(f"  å½’ä¸€åŒ–MSE: {normalized_mse:.6f}")

        # æ–¹æ³•4ï¼šå¯¹æ•°åŸŸMSE
        log_original = np.log1p(original_magnitude)
        log_reconstructed = np.log1p(reconstructed_magnitude)
        log_mse = np.mean((log_reconstructed - log_original) ** 2)
        print(f"  å¯¹æ•°åŸŸMSE: {log_mse:.6f}")

        # 6. ENTè®¡ç®—è¯Šæ–­
        print(f"\nğŸŒŠ ENTè®¡ç®—è¯Šæ–­:")

        # å½“å‰æ–¹æ³•ï¼šEnergyå½’ä¸€åŒ–
        g = np.sqrt(np.sum(reconstructed_magnitude**2))
        print(f"  Energyå½’ä¸€åŒ–å› å­ g: {g:.6f}")

        if g > 1e-10:
            normalized_magnitude = reconstructed_magnitude / g
            entropy_values = normalized_magnitude**2 * np.log(normalized_magnitude**2 + 1e-12)
            current_ent = -np.sum(entropy_values)
            print(f"  å½“å‰ENT (Energyå½’ä¸€åŒ–): {current_ent:.6f}")
        else:
            print(f"  å½“å‰ENT: -inf (èƒ½é‡å¤ªå°)")

        # æ›¿ä»£æ–¹æ³•1ï¼šç›´æ¥å½’ä¸€åŒ–
        if reconstructed_magnitude.max() > 0:
            direct_normalized = reconstructed_magnitude / reconstructed_magnitude.max()
            prob = direct_normalized**2 / np.sum(direct_normalized**2)
            prob = prob[prob > 1e-12]  # é¿å…log(0)
            direct_ent = -np.sum(prob * np.log(prob))
            print(f"  ç›´æ¥å½’ä¸€åŒ–ENT: {direct_ent:.6f}")

        # æ›¿ä»£æ–¹æ³•2ï¼šä¼ ç»Ÿå›¾åƒç†µ
        magnitude_uint8 = ((reconstructed_magnitude / reconstructed_magnitude.max()) * 255).astype(np.uint8)
        hist, _ = np.histogram(magnitude_uint8, bins=256, range=(0, 256), density=True)
        hist = hist[hist > 0]
        traditional_ent = -np.sum(hist * np.log2(hist))
        print(f"  ä¼ ç»Ÿå›¾åƒç†µ: {traditional_ent:.6f}")

        # 7. GTå¯¹æ¯”ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        mat_path = self.analyzer.find_corresponding_mat_file(sar_path)
        if os.path.exists(mat_path):
            print(f"\nğŸ“‹ GTå¯¹æ¯”åˆ†æ:")
            gt_scatterers = self.analyzer.extract_scatterers_from_mat_fixed(mat_path)

            if gt_scatterers:
                gt_reconstructed = reconstruct_sar_image(gt_scatterers)
                gt_magnitude = np.abs(gt_reconstructed)

                print(f"  GTæ•£å°„ä¸­å¿ƒæ•°é‡: {len(gt_scatterers)}")
                print(f"  GTé‡å»ºå›¾åƒå¹…åº¦èŒƒå›´: {gt_magnitude.min():.6f} ~ {gt_magnitude.max():.6f}")

                # GT vs åŸå§‹å›¾åƒçš„MSE
                gt_vs_original = np.linalg.norm(gt_reconstructed - original_complex) / np.linalg.norm(original_complex)
                print(f"  GT vs åŸå§‹å›¾åƒ MSE: {gt_vs_original:.6f}")

                # é¢„æµ‹ vs GTçš„MSE
                pred_vs_gt = np.linalg.norm(reconstructed_complex - gt_reconstructed) / np.linalg.norm(gt_reconstructed)
                print(f"  é¢„æµ‹ vs GT MSE: {pred_vs_gt:.6f}")

        # 8. å¯è§†åŒ–å¯¹æ¯”
        self.create_diagnostic_visualization(original_complex, reconstructed_complex, base_name, predicted_scatterers)

        print(f"\nâœ… è¯Šæ–­å®Œæˆ: {base_name}")

    def create_diagnostic_visualization(self, original, reconstructed, sample_name, scatterers):
        """åˆ›å»ºè¯Šæ–­å¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle(f"Metrics Diagnosis: {sample_name}", fontsize=14)

        original_mag = np.abs(original)
        reconstructed_mag = np.abs(reconstructed)
        difference = reconstructed_mag - original_mag

        # åŸå§‹å›¾åƒ
        im1 = axes[0, 0].imshow(original_mag, cmap="gray")
        axes[0, 0].set_title("Original SAR")
        axes[0, 0].axis("off")
        plt.colorbar(im1, ax=axes[0, 0], fraction=0.046)

        # é‡å»ºå›¾åƒ
        im2 = axes[0, 1].imshow(reconstructed_mag, cmap="gray")
        axes[0, 1].set_title("Reconstructed SAR")
        axes[0, 1].axis("off")
        plt.colorbar(im2, ax=axes[0, 1], fraction=0.046)

        # å·®å¼‚å›¾
        im3 = axes[0, 2].imshow(difference, cmap="seismic", vmin=-difference.max(), vmax=difference.max())
        axes[0, 2].set_title("Difference (Recon - Original)")
        axes[0, 2].axis("off")
        plt.colorbar(im3, ax=axes[0, 2], fraction=0.046)

        # æ•£å°„ä¸­å¿ƒä½ç½®
        axes[1, 0].imshow(original_mag, cmap="gray")
        if scatterers:
            for s in scatterers:
                # å°†ç‰©ç†åæ ‡è½¬æ¢å›åƒç´ åæ ‡è¿›è¡Œæ˜¾ç¤º
                pixel_x = (s["x"] * 128 / (0.3 * 84)) + 65
                pixel_y = 65 - (s["y"] * 128 / (0.3 * 84))
                axes[1, 0].plot(pixel_x, pixel_y, "r+", markersize=8, markeredgewidth=2)
        axes[1, 0].set_title(f"Scatterers ({len(scatterers)})")
        axes[1, 0].axis("off")

        # å¹…åº¦ç›´æ–¹å›¾å¯¹æ¯”
        axes[1, 1].hist(original_mag.flatten(), bins=50, alpha=0.7, label="Original", density=True)
        axes[1, 1].hist(reconstructed_mag.flatten(), bins=50, alpha=0.7, label="Reconstructed", density=True)
        axes[1, 1].set_title("Magnitude Histograms")
        axes[1, 1].legend()
        axes[1, 1].set_xlabel("Magnitude")
        axes[1, 1].set_ylabel("Density")

        # è¯¯å·®åˆ†æ
        error_stats = [
            f"Max Original: {original_mag.max():.4f}",
            f"Max Reconstructed: {reconstructed_mag.max():.4f}",
            f"Mean Original: {original_mag.mean():.4f}",
            f"Mean Reconstructed: {reconstructed_mag.mean():.4f}",
            f"RMS Difference: {np.sqrt(np.mean(difference**2)):.4f}",
        ]

        axes[1, 2].text(
            0.05,
            0.95,
            "\n".join(error_stats),
            transform=axes[1, 2].transAxes,
            fontsize=10,
            verticalalignment="top",
            fontfamily="monospace",
        )
        axes[1, 2].set_title("Error Statistics")
        axes[1, 2].axis("off")

        plt.tight_layout()

        # ä¿å­˜
        output_dir = os.path.join(project_root, "datasets", "result_vis", "diagnosis")
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, f"diagnosis_{sample_name}.png")
        plt.savefig(save_path, dpi=150, bbox_inches="tight")
        plt.close()

        print(f"    ğŸ’¾ è¯Šæ–­å›¾ä¿å­˜: {save_path}")


def main():
    print("ğŸ”¬ å®šé‡æŒ‡æ ‡è®¡ç®—è¯Šæ–­å·¥å…·")
    print("=" * 50)

    # åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½ASC-Netæ¨¡å‹...")
    model = ASCNet_v3_5param(n_channels=1, n_params=5).to(config.DEVICE)
    model_path = os.path.join(config.CHECKPOINT_DIR, config.MODEL_NAME)

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")

    model.load_state_dict(torch.load(model_path, map_location=config.DEVICE))
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    # åŠ è½½æ•°æ®é›†
    dataset = MSTAR_ASC_5CH_Dataset()
    if not dataset.samples:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®é›†æ ·æœ¬")
        return

    print(f"âœ… æ‰¾åˆ° {len(dataset.samples)} ä¸ªæ ·æœ¬")

    # è¯Šæ–­å·¥å…·
    diagnoser = MetricsDiagnosis()

    # è¯Šæ–­å‰5ä¸ªæ ·æœ¬
    print(f"\nğŸ” å¼€å§‹è¯Šæ–­å‰5ä¸ªæ ·æœ¬...")

    for i, sample_info in enumerate(dataset.samples[:5]):
        try:
            sar_path = sample_info["sar"]
            diagnoser.diagnose_single_sample(sar_path, model)
        except Exception as e:
            print(f"âŒ è¯Šæ–­æ ·æœ¬ {i+1} æ—¶å‡ºé”™: {e}")
            import traceback

            traceback.print_exc()

    print(f"\nğŸ‰ è¯Šæ–­å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: datasets/result_vis/diagnosis/")


if __name__ == "__main__":
    main()
