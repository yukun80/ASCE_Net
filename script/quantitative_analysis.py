# script/quantitative_analysis.py - SARé‡å»ºå®šé‡åˆ†æå·¥å…· (Aggressiveæ¨¡å¼)

import os
import sys
import torch
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from scipy import stats
from scipy.ndimage import maximum_filter

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams["font.sans-serif"] = ["SimHei", "Microsoft YaHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# --- Setup Project Path ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from model.asc_net import ASCNet_v3_5param
from utils import config
from utils.dataset import MSTAR_ASC_5CH_Dataset, read_sar_complex_tensor
from utils.reconstruction import (
    extract_scatterers_from_mat,
    extract_scatterers_from_prediction_5ch,
    reconstruct_sar_image,
    pixel_to_model,
)
import scipy.io as sio

os.environ["KMP_DUPLICATE_LIB_OK"] = "True"


class QuantitativeAnalyzer:
    """SARå›¾åƒé‡å»ºå®šé‡åˆ†æå™¨"""

    def __init__(self, model_path=None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        # åŠ è½½æ¨¡å‹
        self.model = ASCNet_v3_5param(n_channels=1, n_params=5).to(config.DEVICE)
        if model_path is None:
            model_path = os.path.join(config.CHECKPOINT_DIR, config.MODEL_NAME)

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")

        self.model.load_state_dict(torch.load(model_path, map_location=config.DEVICE))
        self.model.eval()
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")

        # è®¾ç½®aggressiveæ£€æµ‹é…ç½®ï¼ˆä¸EnhancedVisualizationProcessorä¸€è‡´ï¼‰
        self.detection_config = {
            "low_threshold": 0.05,
            "high_threshold": 0.3,
            "min_distance": 2,
            "amplitude_weight_low": 0.3,
        }

        # åƒç´ é—´è·å¸¸é‡
        self.PIXEL_SPACING = 0.1

        # å­˜å‚¨ç»“æœ
        self.results = {
            "mse_values": [],
            "ent_values": [],
            "sample_names": [],
            "original_images": [],
            "reconstructed_images": [],
            "gt_scatterer_counts": [],
            "pred_scatterer_counts": [],
        }

    def calculate_mse(self, original_complex, reconstructed_complex):
        """
        è®¡ç®—é‡å»ºè¯¯å·®MSE

        å…¬å¼: MSE = sqrt(1/N * sum(||x_hat - x||_2^2 / ||x||_2^2))

        Args:
            original_complex: åŸå§‹SARå¤æ•°å›¾åƒ (H, W)
            reconstructed_complex: é‡å»ºSARå¤æ•°å›¾åƒ (H, W)

        Returns:
            mse: ç›¸å¯¹å‡æ–¹æ ¹è¯¯å·®
        """
        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        if torch.is_tensor(original_complex):
            original_complex = original_complex.cpu().numpy()
        if torch.is_tensor(reconstructed_complex):
            reconstructed_complex = reconstructed_complex.cpu().numpy()

        # è®¡ç®—L2èŒƒæ•°çš„å¹³æ–¹
        diff_norm_sq = np.sum(np.abs(reconstructed_complex - original_complex) ** 2)
        original_norm_sq = np.sum(np.abs(original_complex) ** 2)

        # é¿å…é™¤é›¶
        if original_norm_sq == 0:
            return 0.0

        # ç›¸å¯¹è¯¯å·®
        relative_error_sq = diff_norm_sq / original_norm_sq
        return np.sqrt(relative_error_sq)

    def calculate_entropy(self, image_complex, normalization_method="energy"):
        """
        è®¡ç®—å›¾åƒç†µENT

        å…¬å¼: ENT = -||w/g||_2^2 * ln(||w/g||_2^2)

        Args:
            image_complex: SARå¤æ•°å›¾åƒ (H, W)
            normalization_method: å½’ä¸€åŒ–æ–¹æ³• ('max', 'energy', 'sum')

        Returns:
            entropy: å›¾åƒç†µå€¼
        """
        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        if torch.is_tensor(image_complex):
            image_complex = image_complex.cpu().numpy()

        # è·å–å¹…åº¦
        magnitude = np.abs(image_complex)

        # æ£€æŸ¥æ˜¯å¦ä¸ºå…¨é›¶å›¾åƒ
        if np.all(magnitude == 0):
            print("  âš ï¸ è­¦å‘Š: é‡å»ºå›¾åƒä¸ºå…¨é›¶ï¼ŒENTè®¾ä¸º-inf")
            return float("-inf")

        # å½’ä¸€åŒ–
        if normalization_method == "max":
            g = np.max(magnitude)
        elif normalization_method == "energy":
            g = np.sqrt(np.sum(magnitude**2))
        elif normalization_method == "sum":
            g = np.sum(magnitude)
        else:
            raise ValueError(f"æœªçŸ¥çš„å½’ä¸€åŒ–æ–¹æ³•: {normalization_method}")

        # æ£€æŸ¥å½’ä¸€åŒ–å› å­
        if g <= 1e-12:  # ä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼
            print(f"  âš ï¸ è­¦å‘Š: å½’ä¸€åŒ–å› å­è¿‡å° g={g:.2e}ï¼ŒENTè®¾ä¸º-inf")
            return float("-inf")

        # å½’ä¸€åŒ–æƒé‡
        w_normalized = magnitude / g

        # è®¡ç®—L2èŒƒæ•°çš„å¹³æ–¹
        l2_norm_sq = np.sum(w_normalized**2)

        # æ£€æŸ¥L2èŒƒæ•°
        if l2_norm_sq <= 1e-12:
            print(f"  âš ï¸ è­¦å‘Š: L2èŒƒæ•°è¿‡å° {l2_norm_sq:.2e}ï¼ŒENTè®¾ä¸º-inf")
            return float("-inf")

        # è®¡ç®—ç†µ
        entropy = -l2_norm_sq * np.log(l2_norm_sq)

        # æ£€æŸ¥ç»“æœæ˜¯å¦åˆç†
        if np.isnan(entropy) or np.isinf(entropy):
            print(f"  âš ï¸ è­¦å‘Š: ENTè®¡ç®—å¼‚å¸¸ {entropy}ï¼ŒL2èŒƒæ•°={l2_norm_sq:.6f}")
            return float("-inf")

        return entropy

    def find_corresponding_mat_file(self, sar_path):
        """æŸ¥æ‰¾å¯¹åº”çš„MATæ–‡ä»¶"""
        mat_root = os.path.join(project_root, "datasets", "SAR_ASC_Project", "tmp_Training_ASC")
        base_name = os.path.basename(sar_path).replace(".raw", "")
        return os.path.join(mat_root, f"{base_name}.mat")

    def extract_scatterers_from_mat_fixed(self, mat_path):
        """ä¿®å¤ç‰ˆï¼šæ­£ç¡®è§£æåŒé‡åµŒå¥—çš„MATæ–‡ä»¶ç»“æ„"""
        scatterers = []
        try:
            mat_data = sio.loadmat(mat_path)
            scatter_all = mat_data["scatter_all"]

            print(f"    ğŸ“Š MATç»“æ„: {scatter_all.shape}, ç±»å‹: {type(scatter_all)}")

            # æ ¹æ®ç”¨æˆ·æè¿°ï¼šscatter_allæ˜¯Nx1 cellï¼Œæ¯ä¸ªå…ƒç´ æ˜¯1x1 cellï¼ŒåŒ…å«å‚æ•°æ•°ç»„
            for i in range(scatter_all.shape[0]):
                try:
                    # ç¬¬ä¸€å±‚ï¼šscatter_all[i, 0] è·å–ç¬¬iä¸ªcell
                    cell = scatter_all[i, 0]

                    # ç¬¬äºŒå±‚ï¼šcell[0, 0] è·å–å†…å±‚çš„å‚æ•°æ•°ç»„
                    params = cell[0, 0]

                    if isinstance(params, np.ndarray) and len(params) >= 7:
                        # ç”¨æˆ·ç¤ºä¾‹ï¼š[2.1672 0.3851 0.5000 5.9223e-04 0 0 2.6466]
                        # å‚æ•°é¡ºåºï¼š[x, y, alpha, gamma, phi_prime, L, A]
                        scatterers.append(
                            {
                                "x": float(params[0]),
                                "y": float(params[1]),
                                "alpha": float(params[2]),
                                "gamma": float(params[3]),
                                "phi_prime": float(params[4]),
                                "L": float(params[5]),
                                "A": float(params[6]),
                            }
                        )
                    else:
                        print(f"    âš ï¸ ç¬¬{i+1}ä¸ªæ•£å°„ä¸­å¿ƒå‚æ•°å¼‚å¸¸: {type(params)}, {params}")

                except (IndexError, TypeError) as e:
                    print(f"    âš ï¸ è§£æç¬¬{i+1}ä¸ªæ•£å°„ä¸­å¿ƒé”™è¯¯: {e}")
                    continue

            print(f"    âœ… æ­£ç¡®æå– {len(scatterers)} ä¸ªæ•£å°„ä¸­å¿ƒ")

            # æ˜¾ç¤ºæ•£å°„ä¸­å¿ƒç»Ÿè®¡
            if scatterers:
                amplitudes = [s["A"] for s in scatterers]
                print(f"    ğŸ“ˆ å¹…åº¦èŒƒå›´: {min(amplitudes):.4f} ~ {max(amplitudes):.4f}")
                print(f"    ğŸ“ˆ å¹³å‡å¹…åº¦: {np.mean(amplitudes):.4f}")

        except Exception as e:
            print(f"    âŒ MATè§£æé”™è¯¯: {e}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            try:
                scatterers = extract_scatterers_from_mat(mat_path)
                print(f"    ğŸ”„ å›é€€æ–¹æ³•æå– {len(scatterers)} ä¸ªæ•£å°„ä¸­å¿ƒ")
            except:
                pass

        return scatterers

    def extract_scatterers_aggressive(self, prediction_maps):
        """
        ä½¿ç”¨aggressiveé…ç½®æå–æ•£å°„ä¸­å¿ƒ

        Args:
            prediction_maps: æ¨¡å‹é¢„æµ‹çš„5é€šé“è¾“å‡º [heatmap, A_log1p, alpha, dx, dy]

        Returns:
            scatterers: æ•£å°„ä¸­å¿ƒåˆ—è¡¨
        """
        heatmap, pred_A_log1p, pred_alpha, pred_dx, pred_dy = [prediction_maps[i] for i in range(5)]
        pred_A = np.expm1(pred_A_log1p)

        # éæœ€å¤§å€¼æŠ‘åˆ¶
        neighborhood_size = 5
        local_max = maximum_filter(heatmap, size=neighborhood_size) == heatmap

        # é«˜é˜ˆå€¼æ£€æµ‹
        high_candidates = (heatmap > self.detection_config["high_threshold"]) & local_max
        high_peak_coords = np.argwhere(high_candidates)

        # ä½é˜ˆå€¼æ£€æµ‹
        low_candidates = (heatmap > self.detection_config["low_threshold"]) & local_max
        low_peak_coords = np.argwhere(low_candidates)

        # åˆå¹¶ç»“æœï¼Œé¿å…é‡å¤
        all_coords = []
        for coord in high_peak_coords:
            all_coords.append((coord[0], coord[1], "high"))

        for coord in low_peak_coords:
            is_duplicate = False
            for existing_coord in high_peak_coords:
                if np.linalg.norm(coord - existing_coord) < self.detection_config["min_distance"]:
                    is_duplicate = True
                    break
            if not is_duplicate:
                all_coords.append((coord[0], coord[1], "low"))

        scatterers = []
        for r, c, strength in all_coords:
            amplitude_weight = 1.0 if strength == "high" else self.detection_config["amplitude_weight_low"]

            x_base, y_base = pixel_to_model(r, c)
            dx = pred_dx[r, c] * self.PIXEL_SPACING
            dy = pred_dy[r, c] * self.PIXEL_SPACING
            x_final = x_base + dx
            y_final = y_base - dy

            A_final = pred_A[r, c] * amplitude_weight
            alpha_final = pred_alpha[r, c]

            scatterers.append(
                {
                    "A": A_final,
                    "alpha": alpha_final,
                    "x": x_final,
                    "y": y_final,
                    "gamma": 0,
                    "L": 0,
                    "phi_prime": 0,
                    "strength": strength,
                }
            )

        return scatterers

    def analyze_sample(self, sample_info, threshold=0.5):
        """
        åˆ†æå•ä¸ªæ ·æœ¬

        Args:
            sample_info: æ ·æœ¬ä¿¡æ¯å­—å…¸
            threshold: æ•£å°„ä¸­å¿ƒæ£€æµ‹é˜ˆå€¼

        Returns:
            result_dict: åˆ†æç»“æœå­—å…¸
        """
        sar_path = sample_info["sar"]
        base_name = os.path.basename(sar_path).replace(".raw", "")

        try:
            # 1. åŠ è½½åŸå§‹SARæ•°æ®
            original_sar_tensor = read_sar_complex_tensor(sar_path, config.IMG_HEIGHT, config.IMG_WIDTH)
            if original_sar_tensor is None:
                print(f"  âœ— æ— æ³•åŠ è½½SARæ•°æ®: {base_name}")
                return None

            # ç§»é™¤batchç»´åº¦ï¼Œè·å–å¤æ•°æ•°æ®
            original_complex = original_sar_tensor.squeeze(0)  # [H, W] complex tensor

            # 2. æ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                input_tensor = original_sar_tensor.unsqueeze(0).to(config.DEVICE)
                predicted_maps = self.model(input_tensor).squeeze(0).cpu().numpy()

            # 3. ä½¿ç”¨aggressiveé…ç½®æå–é¢„æµ‹çš„æ•£å°„ä¸­å¿ƒ
            predicted_scatterers = self.extract_scatterers_aggressive(predicted_maps)

            # 4. é‡å»ºSARå›¾åƒ
            if predicted_scatterers:
                reconstructed_complex = reconstruct_sar_image(predicted_scatterers)
                recon_energy = np.sum(np.abs(reconstructed_complex) ** 2)
                print(f"    é‡å»ºå›¾åƒèƒ½é‡: {recon_energy:.6f}")
            else:
                print(f"    âš ï¸ æœªæ£€æµ‹åˆ°æ•£å°„ä¸­å¿ƒï¼Œé‡å»ºå›¾åƒä¸ºå…¨é›¶")
                reconstructed_complex = np.zeros_like(original_complex.cpu().numpy())

            # 5. è®¡ç®—MSE
            mse_value = self.calculate_mse(original_complex, reconstructed_complex)

            # 6. è®¡ç®—ENT (å¯¹é‡å»ºå›¾åƒï¼Œä½¿ç”¨energyå½’ä¸€åŒ–)
            ent_value = self.calculate_entropy(reconstructed_complex, normalization_method="energy")

            # 7. è·å–GTæ•£å°„ä¸­å¿ƒï¼ˆä½¿ç”¨ä¿®å¤çš„è§£ææ–¹æ³•ï¼‰
            gt_count = 0
            gt_scatterers = []
            mat_path = self.find_corresponding_mat_file(sar_path)
            if os.path.exists(mat_path):
                print(f"    ğŸ” è§£æGTæ•£å°„ä¸­å¿ƒ: {os.path.basename(mat_path)}")
                gt_scatterers = self.extract_scatterers_from_mat_fixed(mat_path)
                gt_count = len(gt_scatterers)

                # å¦‚æœä¿®å¤æ–¹æ³•å¤±è´¥ï¼Œå°è¯•åŸå§‹æ–¹æ³•
                if gt_count == 0:
                    try:
                        gt_scatterers = extract_scatterers_from_mat(mat_path)
                        gt_count = len(gt_scatterers)
                        print(f"    ğŸ”„ ä½¿ç”¨åŸå§‹æ–¹æ³•è·å– {gt_count} ä¸ªGTæ•£å°„ä¸­å¿ƒ")
                    except:
                        print(f"    âŒ æ— æ³•è§£æGTæ•£å°„ä¸­å¿ƒ")
            else:
                print(f"    âš ï¸ MATæ–‡ä»¶ä¸å­˜åœ¨: {os.path.basename(mat_path)}")

            # 8. å¯é€‰ï¼šä½¿ç”¨GTæ•£å°„ä¸­å¿ƒé‡å»ºè¿›è¡Œå¯¹æ¯”
            gt_reconstructed = None
            if gt_scatterers:
                try:
                    gt_reconstructed = reconstruct_sar_image(gt_scatterers)
                    print(f"    âœ… GTé‡å»ºå›¾åƒç”ŸæˆæˆåŠŸ")
                except Exception as e:
                    print(f"    âš ï¸ GTé‡å»ºå¤±è´¥: {e}")

            result = {
                "sample_name": base_name,
                "mse": mse_value,
                "entropy": ent_value,
                "original_image": original_complex.cpu().numpy(),
                "reconstructed_image": reconstructed_complex,
                "gt_scatterer_count": gt_count,
                "pred_scatterer_count": len(predicted_scatterers),
                "reconstruction_ratio": len(predicted_scatterers) / max(gt_count, 1),
            }

            print(
                f"  âœ“ {base_name}: MSE={mse_value:.6f}, ENT={ent_value:.6f}, "
                f"æ•£å°„ä¸­å¿ƒ={len(predicted_scatterers)}/{gt_count} (æ£€å‡ºç‡:{len(predicted_scatterers)/max(gt_count,1):.2%})"
            )

            return result

        except Exception as e:
            print(f"  âœ— åˆ†æé”™è¯¯ {base_name}: {e}")
            return None

    def batch_analyze(self, dataset, max_samples=None):
        """
        æ‰¹é‡åˆ†ææ•°æ®é›†

        Args:
            dataset: MSTARæ•°æ®é›†
            max_samples: æœ€å¤§åˆ†ææ ·æœ¬æ•°
        """
        print(f"\n=== å¼€å§‹æ‰¹é‡å®šé‡åˆ†æ ===")
        print(f"ğŸ“‹ åˆ†æé…ç½®:")
        print(f"  â€¢ æ•£å°„ä¸­å¿ƒæ£€æµ‹: Aggressiveæ¨¡å¼")
        print(f"    - ä½é˜ˆå€¼: {self.detection_config['low_threshold']}")
        print(f"    - é«˜é˜ˆå€¼: {self.detection_config['high_threshold']}")
        print(f"    - æœ€å°è·ç¦»: {self.detection_config['min_distance']}")
        print(f"    - å¼±æ•£å°„ä¸­å¿ƒæƒé‡: {self.detection_config['amplitude_weight_low']}")
        print(f"  â€¢ MSEè®¡ç®—: ç›¸å¯¹å‡æ–¹æ ¹è¯¯å·®")
        print(f"  â€¢ ENTè®¡ç®—: Energyå½’ä¸€åŒ–æ–¹æ³•")

        samples_to_analyze = len(dataset.samples)
        if max_samples:
            samples_to_analyze = min(max_samples, samples_to_analyze)

        print(f"\nåˆ†ææ ·æœ¬æ•°: {samples_to_analyze}")

        successful_count = 0

        for i, sample_info in enumerate(tqdm(dataset.samples[:samples_to_analyze], desc="åˆ†æè¿›åº¦")):
            result = self.analyze_sample(sample_info)

            if result:
                # å­˜å‚¨ç»“æœ
                self.results["mse_values"].append(result["mse"])
                self.results["ent_values"].append(result["entropy"])
                self.results["sample_names"].append(result["sample_name"])
                self.results["original_images"].append(result["original_image"])
                self.results["reconstructed_images"].append(result["reconstructed_image"])
                self.results["gt_scatterer_counts"].append(result["gt_scatterer_count"])
                self.results["pred_scatterer_counts"].append(result["pred_scatterer_count"])

                successful_count += 1

        print(f"\n=== åˆ†æå®Œæˆ ===")
        print(f"æˆåŠŸåˆ†æ: {successful_count}/{samples_to_analyze} ä¸ªæ ·æœ¬")

        if successful_count > 0:
            self.generate_statistics()

    def generate_statistics(self):
        """ç”Ÿæˆç»Ÿè®¡ç»“æœ"""
        mse_values = np.array(self.results["mse_values"])
        ent_values = np.array(self.results["ent_values"])

        print(f"\nğŸ“Š å®šé‡åˆ†æç»Ÿè®¡ç»“æœ (Aggressiveæ¨¡å¼):")
        print(f"{'='*60}")

        print(f"ğŸ¯ MSE (é‡å»ºè¯¯å·®) - ç›¸å¯¹å‡æ–¹æ ¹è¯¯å·®:")
        print(f"  æ ·æœ¬æ•°é‡:     {len(mse_values)}")
        print(f"  å¹³å‡å€¼:       {np.mean(mse_values):.6f}")
        print(f"  æ ‡å‡†å·®:       {np.std(mse_values):.6f}")
        print(f"  ä¸­ä½æ•°:       {np.median(mse_values):.6f}")
        print(f"  æœ€å°å€¼:       {np.min(mse_values):.6f}")
        print(f"  æœ€å¤§å€¼:       {np.max(mse_values):.6f}")
        print(f"  25ç™¾åˆ†ä½:     {np.percentile(mse_values, 25):.6f}")
        print(f"  75ç™¾åˆ†ä½:     {np.percentile(mse_values, 75):.6f}")

        # å¤„ç†ENTå€¼ä¸­çš„-inf
        valid_ent = ent_values[~np.isinf(ent_values)]
        inf_count = np.sum(np.isinf(ent_values))

        print(f"\nğŸŒŠ ENT (å›¾åƒç†µ) - Energyå½’ä¸€åŒ–:")
        print(f"  æ ·æœ¬æ•°é‡:     {len(ent_values)}")
        print(f"  æœ‰æ•ˆæ ·æœ¬:     {len(valid_ent)}")
        print(f"  æ— æ•ˆæ ·æœ¬(-inf):{inf_count}")

        if len(valid_ent) > 0:
            print(f"  å¹³å‡å€¼:       {np.mean(valid_ent):.6f}")
            print(f"  æ ‡å‡†å·®:       {np.std(valid_ent):.6f}")
            print(f"  ä¸­ä½æ•°:       {np.median(valid_ent):.6f}")
            print(f"  æœ€å°å€¼:       {np.min(valid_ent):.6f}")
            print(f"  æœ€å¤§å€¼:       {np.max(valid_ent):.6f}")
            print(f"  25ç™¾åˆ†ä½:     {np.percentile(valid_ent, 25):.6f}")
            print(f"  75ç™¾åˆ†ä½:     {np.percentile(valid_ent, 75):.6f}")
        else:
            print(f"  âš ï¸ æ‰€æœ‰ENTå€¼å‡ä¸º-inf (é‡å»ºå›¾åƒèƒ½é‡è¿‡ä½)")

        # æ•£å°„ä¸­å¿ƒæ£€å‡ºç‡ç»Ÿè®¡
        gt_counts = np.array(self.results["gt_scatterer_counts"])
        pred_counts = np.array(self.results["pred_scatterer_counts"])
        detection_ratios = pred_counts / np.maximum(gt_counts, 1)

        print(f"\nğŸ¯ æ•£å°„ä¸­å¿ƒæ£€å‡ºç»Ÿè®¡ (Aggressiveæ¨¡å¼):")
        print(f"  GTå¹³å‡æ•°é‡:   {np.mean(gt_counts):.2f}")
        print(f"  é¢„æµ‹å¹³å‡æ•°é‡: {np.mean(pred_counts):.2f}")
        print(f"  å¹³å‡æ£€å‡ºç‡:   {np.mean(detection_ratios):.2%}")
        print(f"  æ£€å‡ºç‡ä¸­ä½æ•°: {np.median(detection_ratios):.2%}")
        print(f"  æ£€å‡ºç‡æ ‡å‡†å·®: {np.std(detection_ratios):.2%}")
        print(
            f"  å®Œç¾æ£€å‡ºæ ·æœ¬: {np.sum(detection_ratios >= 0.95)}/{len(detection_ratios)} ({np.sum(detection_ratios >= 0.95)/len(detection_ratios):.1%})"
        )
        print(
            f"  è‰¯å¥½æ£€å‡ºæ ·æœ¬: {np.sum(detection_ratios >= 0.8)}/{len(detection_ratios)} ({np.sum(detection_ratios >= 0.8)/len(detection_ratios):.1%})"
        )

        print(f"\nğŸ“ˆ æ€§èƒ½æ±‡æ€»:")
        print(
            f"  MSE < 0.1çš„æ ·æœ¬: {np.sum(mse_values < 0.1)}/{len(mse_values)} ({np.sum(mse_values < 0.1)/len(mse_values):.1%})"
        )
        print(
            f"  MSE < 0.2çš„æ ·æœ¬: {np.sum(mse_values < 0.2)}/{len(mse_values)} ({np.sum(mse_values < 0.2)/len(mse_values):.1%})"
        )
        print(
            f"  ENT > -1çš„æ ·æœ¬:  {np.sum(ent_values > -1)}/{len(ent_values)} ({np.sum(ent_values > -1)/len(ent_values):.1%})"
        )
        print(f"{'='*60}")

    def save_results(self, output_dir):
        """ä¿å­˜åˆ†æç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)

        # 1. ä¿å­˜ç»Ÿè®¡æ•°æ®
        stats_file = os.path.join(output_dir, "quantitative_statistics.txt")
        with open(stats_file, "w", encoding="utf-8") as f:
            mse_values = np.array(self.results["mse_values"])
            ent_values = np.array(self.results["ent_values"])

            f.write("SARå›¾åƒé‡å»ºå®šé‡åˆ†æç»“æœ (Aggressiveæ¨¡å¼)\n")
            f.write("=" * 60 + "\n\n")

            f.write("åˆ†æé…ç½®:\n")
            f.write(f"  æ•£å°„ä¸­å¿ƒæ£€æµ‹: Aggressiveæ¨¡å¼\n")
            f.write(f"  ENTå½’ä¸€åŒ–æ–¹æ³•: Energy\n\n")

            f.write("MSE (é‡å»ºè¯¯å·®) ç»Ÿè®¡ - ç›¸å¯¹å‡æ–¹æ ¹è¯¯å·®:\n")
            f.write(f"  æ ·æœ¬æ•°é‡: {len(mse_values)}\n")
            f.write(f"  å¹³å‡å€¼: {np.mean(mse_values):.6f}\n")
            f.write(f"  æ ‡å‡†å·®: {np.std(mse_values):.6f}\n")
            f.write(f"  ä¸­ä½æ•°: {np.median(mse_values):.6f}\n")
            f.write(f"  æœ€å°å€¼: {np.min(mse_values):.6f}\n")
            f.write(f"  æœ€å¤§å€¼: {np.max(mse_values):.6f}\n")
            f.write(f"  25ç™¾åˆ†ä½: {np.percentile(mse_values, 25):.6f}\n")
            f.write(f"  75ç™¾åˆ†ä½: {np.percentile(mse_values, 75):.6f}\n\n")

            # å¤„ç†ENTå€¼ä¸­çš„-inf
            valid_ent = ent_values[~np.isinf(ent_values)]
            inf_count = np.sum(np.isinf(ent_values))

            f.write("ENT (å›¾åƒç†µ) ç»Ÿè®¡ - Energyå½’ä¸€åŒ–:\n")
            f.write(f"  æ ·æœ¬æ•°é‡: {len(ent_values)}\n")
            f.write(f"  æœ‰æ•ˆæ ·æœ¬: {len(valid_ent)}\n")
            f.write(f"  æ— æ•ˆæ ·æœ¬(-inf): {inf_count}\n")

            if len(valid_ent) > 0:
                f.write(f"  å¹³å‡å€¼: {np.mean(valid_ent):.6f}\n")
                f.write(f"  æ ‡å‡†å·®: {np.std(valid_ent):.6f}\n")
                f.write(f"  ä¸­ä½æ•°: {np.median(valid_ent):.6f}\n")
                f.write(f"  æœ€å°å€¼: {np.min(valid_ent):.6f}\n")
                f.write(f"  æœ€å¤§å€¼: {np.max(valid_ent):.6f}\n")
                f.write(f"  25ç™¾åˆ†ä½: {np.percentile(valid_ent, 25):.6f}\n")
                f.write(f"  75ç™¾åˆ†ä½: {np.percentile(valid_ent, 75):.6f}\n")
            else:
                f.write("  âš ï¸ æ‰€æœ‰ENTå€¼å‡ä¸º-inf (é‡å»ºå›¾åƒèƒ½é‡è¿‡ä½)\n")

        # 2. ä¿å­˜è¯¦ç»†æ•°æ®
        detailed_file = os.path.join(output_dir, "detailed_results.txt")
        with open(detailed_file, "w", encoding="utf-8") as f:
            f.write("æ ·æœ¬åç§°\tMSE\tENT\tGTæ•£å°„ä¸­å¿ƒ\té¢„æµ‹æ•£å°„ä¸­å¿ƒ\tæ£€å‡ºç‡\n")
            for i, name in enumerate(self.results["sample_names"]):
                gt_count = self.results["gt_scatterer_counts"][i]
                pred_count = self.results["pred_scatterer_counts"][i]
                ratio = pred_count / max(gt_count, 1)

                f.write(
                    f"{name}\t{self.results['mse_values'][i]:.6f}\t"
                    f"{self.results['ent_values'][i]:.6f}\t{gt_count}\t"
                    f"{pred_count}\t{ratio:.3f}\n"
                )

        # 3. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.plot_statistics(output_dir)

        print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

    def plot_statistics(self, output_dir):
        """ç”Ÿæˆç»Ÿè®¡å›¾è¡¨"""
        mse_values = np.array(self.results["mse_values"])
        ent_values = np.array(self.results["ent_values"])

        # å¤„ç†ENTä¸­çš„-infå€¼
        valid_ent = ent_values[~np.isinf(ent_values)]
        if len(valid_ent) == 0:
            # å¦‚æœæ‰€æœ‰ENTéƒ½æ˜¯-infï¼Œç”¨ä¸€ä¸ªå¾ˆå°çš„è´Ÿæ•°æ›¿ä»£ä»¥ä¾¿ç»˜å›¾
            ent_values = np.where(np.isinf(ent_values), -10, ent_values)

        # åˆ›å»º2x2å­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle("SAR Image Reconstruction Quantitative Analysis Results", fontsize=16)

        # MSEç›´æ–¹å›¾
        axes[0, 0].hist(mse_values, bins=30, alpha=0.7, color="blue", edgecolor="black")
        axes[0, 0].set_title("MSE Distribution")
        axes[0, 0].set_xlabel("MSE Value")
        axes[0, 0].set_ylabel("Frequency")
        axes[0, 0].grid(True, alpha=0.3)

        # ENTç›´æ–¹å›¾
        axes[0, 1].hist(ent_values, bins=30, alpha=0.7, color="green", edgecolor="black")
        axes[0, 1].set_title("ENT Distribution")
        axes[0, 1].set_xlabel("ENT Value")
        axes[0, 1].set_ylabel("Frequency")
        axes[0, 1].grid(True, alpha=0.3)

        # MSE vs ENTæ•£ç‚¹å›¾
        axes[1, 0].scatter(mse_values, ent_values, alpha=0.6, color="red")
        axes[1, 0].set_title("MSE vs ENT Relationship")
        axes[1, 0].set_xlabel("MSE Value")
        axes[1, 0].set_ylabel("ENT Value")
        axes[1, 0].grid(True, alpha=0.3)

        # æ•£å°„ä¸­å¿ƒæ£€å‡ºç‡
        gt_counts = np.array(self.results["gt_scatterer_counts"])
        pred_counts = np.array(self.results["pred_scatterer_counts"])

        axes[1, 1].scatter(gt_counts, pred_counts, alpha=0.6, color="purple")
        axes[1, 1].plot([0, max(gt_counts)], [0, max(gt_counts)], "r--", label="Ideal Line")
        axes[1, 1].set_title("Scatterer Detection Comparison")
        axes[1, 1].set_xlabel("GT Scatterer Count")
        axes[1, 1].set_ylabel("Predicted Scatterer Count")
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "quantitative_analysis.png"), dpi=300, bbox_inches="tight")
        plt.close()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸš€ SARå›¾åƒé‡å»ºå®šé‡åˆ†æå·¥å…· - Aggressiveæ¨¡å¼")
    print("=" * 70)
    print("ğŸ“‹ åˆ†ææŒ‡æ ‡:")
    print("  â€¢ MSE: é‡å»ºè¯¯å·® (ç›¸å¯¹å‡æ–¹æ ¹è¯¯å·®)")
    print("  â€¢ ENT: å›¾åƒç†µ (Energyå½’ä¸€åŒ–)")
    print("  â€¢ æ•£å°„ä¸­å¿ƒæ£€å‡ºç‡ç»Ÿè®¡")
    print("-" * 70)

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = QuantitativeAnalyzer()

    # åŠ è½½æ•°æ®é›†
    dataset = MSTAR_ASC_5CH_Dataset()
    if not dataset.samples:
        print("âœ— æœªæ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬")
        return

    print(f"âœ“ æ‰¾åˆ° {len(dataset.samples)} ä¸ªæ ·æœ¬")

    # è¿›è¡Œæ‰¹é‡åˆ†æï¼ˆå¯ä»¥è®¾ç½®max_samplesé™åˆ¶æ ·æœ¬æ•°ï¼‰
    analyzer.batch_analyze(dataset, max_samples=100)  # åˆ†æå‰100ä¸ªæ ·æœ¬

    # ä¿å­˜ç»“æœ
    output_dir = os.path.join(project_root, "datasets", "result_vis", "quantitative_analysis")
    analyzer.save_results(output_dir)

    print(f"\nğŸ‰ å®šé‡åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“Š ç”Ÿæˆæ–‡ä»¶:")
    print(f"  â€¢ quantitative_statistics.txt - è¯¦ç»†ç»Ÿè®¡æ•°æ®")
    print(f"  â€¢ detailed_results.txt - æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ")
    print(f"  â€¢ quantitative_analysis.png - å¯è§†åŒ–å›¾è¡¨")
    print("=" * 70)


if __name__ == "__main__":
    main()
