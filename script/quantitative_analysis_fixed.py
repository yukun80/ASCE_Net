#!/usr/bin/env python
# script/quantitative_analysis_fixed.py - ä¿®å¤ç‰ˆå®šé‡åˆ†æå·¥å…·

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import scipy.io as sio
from tqdm import tqdm
from scipy.ndimage import maximum_filter

# Setup Project Path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from model.asc_net import ASCNet_v3_5param
from utils import config
from utils.dataset import MSTAR_ASC_5CH_Dataset, read_sar_complex_tensor
from utils.reconstruction import (
    extract_scatterers_from_mat,
    extract_scatterers_from_prediction_5ch,
    reconstruct_sar_image,
    pixel_to_model,
)

os.environ["KMP_DUPLICATE_LIB_OK"] = "True"

# è®¾ç½®å­—ä½“ä»¥é¿å…è­¦å‘Š
matplotlib.rc("font", family=["SimHei", "Microsoft YaHei", "DejaVu Sans"])
matplotlib.rc("axes", unicode_minus=False)


class QuantitativeAnalyzerFixed:
    """ä¿®å¤ç‰ˆå®šé‡åˆ†æå™¨"""

    def __init__(self):
        # Aggressiveæ¨¡å¼æ£€æµ‹é…ç½® - æ ¹æ®ç”¨æˆ·è¦æ±‚
        self.detection_config = {
            "low_threshold": 0.05,
            "high_threshold": 0.3,
            "min_distance": 2,
            "amplitude_weight_low": 0.3,
        }

        # åƒç´ é—´è·å¸¸é‡
        self.IMG_HEIGHT = 128
        self.IMG_WIDTH = 128
        self.P_GRID_SIZE = 84
        self.C1 = self.IMG_HEIGHT / (0.3 * self.P_GRID_SIZE)
        self.PIXEL_SPACING = 0.1

        print("ğŸ”§ å®šé‡åˆ†æå™¨é…ç½® (ä¿®å¤ç‰ˆ):")
        print(f"  æ£€æµ‹æ¨¡å¼: Aggressive")
        print(f"  ä½é˜ˆå€¼: {self.detection_config['low_threshold']}")
        print(f"  é«˜é˜ˆå€¼: {self.detection_config['high_threshold']}")
        print(f"  æœ€å°è·ç¦»: {self.detection_config['min_distance']}")
        print(f"  å¼±æ•£å°„åŠ æƒ: {self.detection_config['amplitude_weight_low']}")

    def extract_scatterers_from_mat_corrected(self, mat_path):
        """ä¿®å¤ç‰ˆï¼šæ­£ç¡®è§£æç”¨æˆ·æä¾›çš„MATæ–‡ä»¶ç»“æ„"""
        scatterers = []
        try:
            mat_data = sio.loadmat(mat_path)
            scatter_all = mat_data["scatter_all"]

            print(f"    ğŸ“Š MATç»“æ„: {scatter_all.shape}")

            # ç”¨æˆ·æè¿°ï¼šscatter_allæ˜¯Nx1 cellï¼Œæ¯ä¸ªå…ƒç´ æ˜¯1x1 cellï¼ŒåŒ…å«å‚æ•°æ•°ç»„
            for i in range(scatter_all.shape[0]):
                try:
                    # ç¬¬ä¸€å±‚ï¼šscatter_all[i, 0] è·å–ç¬¬iä¸ªcell
                    cell = scatter_all[i, 0]

                    # ç¬¬äºŒå±‚ï¼šcell[0, 0] è·å–å†…å±‚çš„å‚æ•°æ•°ç»„
                    params = cell[0, 0]

                    if isinstance(params, np.ndarray) and len(params) >= 7:
                        # ç”¨æˆ·ç¤ºä¾‹ï¼š[2.1672 0.3851 0.5000 5.9223e-04 0 0 2.6466]
                        # å‚æ•°é¡ºåºï¼š[x, y, alpha, gamma, phi_prime, L, A]
                        scatterers.append(
                            {
                                "x": float(params[0]),
                                "y": float(params[1]),
                                "alpha": float(params[2]),
                                "gamma": float(params[3]),
                                "phi_prime": float(params[4]),
                                "L": float(params[5]),
                                "A": float(params[6]),
                            }
                        )
                    else:
                        print(f"    âš ï¸ ç¬¬{i+1}ä¸ªå‚æ•°å¼‚å¸¸: {type(params)}")

                except (IndexError, TypeError) as e:
                    continue

            print(f"    âœ… æˆåŠŸæå– {len(scatterers)} ä¸ªGTæ•£å°„ä¸­å¿ƒ")

            # æ˜¾ç¤ºGTæ•£å°„ä¸­å¿ƒç»Ÿè®¡
            if scatterers:
                amplitudes = [s["A"] for s in scatterers]
                x_coords = [s["x"] for s in scatterers]
                y_coords = [s["y"] for s in scatterers]
                print(f"    ğŸ“ˆ å¹…åº¦èŒƒå›´: {min(amplitudes):.4f} ~ {max(amplitudes):.4f}")
                print(f"    ğŸ“ˆ Xåæ ‡èŒƒå›´: {min(x_coords):.4f} ~ {max(x_coords):.4f}")
                print(f"    ğŸ“ˆ Yåæ ‡èŒƒå›´: {min(y_coords):.4f} ~ {max(y_coords):.4f}")

        except Exception as e:
            print(f"    âŒ MATè§£æé”™è¯¯: {e}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            try:
                scatterers = extract_scatterers_from_mat(mat_path)
                print(f"    ğŸ”„ å›é€€æ–¹æ³•æå– {len(scatterers)} ä¸ªæ•£å°„ä¸­å¿ƒ")
            except:
                pass

        return scatterers

    def extract_scatterers_aggressive(self, prediction_maps):
        """Aggressiveæ¨¡å¼æ•£å°„ä¸­å¿ƒæå–"""
        heatmap, pred_A_log1p, pred_alpha, pred_dx, pred_dy = [prediction_maps[i] for i in range(5)]
        pred_A = np.expm1(pred_A_log1p)

        # éæœ€å¤§å€¼æŠ‘åˆ¶
        neighborhood_size = 5
        local_max = maximum_filter(heatmap, size=neighborhood_size) == heatmap

        # é«˜é˜ˆå€¼æ£€æµ‹
        high_candidates = (heatmap > self.detection_config["high_threshold"]) & local_max
        high_peak_coords = np.argwhere(high_candidates)

        # ä½é˜ˆå€¼æ£€æµ‹
        low_candidates = (heatmap > self.detection_config["low_threshold"]) & local_max
        low_peak_coords = np.argwhere(low_candidates)

        # åˆå¹¶ç»“æœï¼Œé¿å…é‡å¤
        all_coords = []
        for coord in high_peak_coords:
            all_coords.append((coord[0], coord[1], "high"))

        for coord in low_peak_coords:
            is_duplicate = False
            for existing_coord in high_peak_coords:
                if np.linalg.norm(coord - existing_coord) < self.detection_config["min_distance"]:
                    is_duplicate = True
                    break
            if not is_duplicate:
                all_coords.append((coord[0], coord[1], "low"))

        scatterers = []
        for r, c, strength in all_coords:
            amplitude_weight = 1.0 if strength == "high" else self.detection_config["amplitude_weight_low"]

            x_base, y_base = pixel_to_model(r, c)
            dx = pred_dx[r, c] * self.PIXEL_SPACING
            dy = pred_dy[r, c] * self.PIXEL_SPACING
            x_final = x_base + dx
            y_final = y_base - dy

            A_final = pred_A[r, c] * amplitude_weight
            alpha_final = pred_alpha[r, c]

            scatterers.append(
                {
                    "A": A_final,
                    "alpha": alpha_final,
                    "x": x_final,
                    "y": y_final,
                    "gamma": 0,
                    "L": 0,
                    "phi_prime": 0,
                    "strength": strength,
                }
            )

        return scatterers

    def calculate_mse_corrected(self, original_complex, reconstructed_complex):
        """ä¿®å¤ç‰ˆMSEè®¡ç®— - ä½¿ç”¨ç›¸å¯¹RMSE"""
        original_norm = np.linalg.norm(original_complex)
        if original_norm == 0:
            return float("inf")

        diff_norm = np.linalg.norm(reconstructed_complex - original_complex)
        relative_rmse = diff_norm / original_norm

        return relative_rmse

    def calculate_entropy_corrected(self, image_complex):
        """ä¿®å¤ç‰ˆENTè®¡ç®— - å¤šç§æ–¹æ³•å¯¹æ¯”"""
        magnitude = np.abs(image_complex)

        # æ–¹æ³•1ï¼šEnergyå½’ä¸€åŒ– (åŸå§‹æ–¹æ³•)
        g = np.sqrt(np.sum(magnitude**2))
        if g > 1e-10:
            normalized_magnitude = magnitude / g
            # ä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒå½¢å¼
            prob_density = normalized_magnitude**2
            # é¿å…log(0)
            prob_density = prob_density[prob_density > 1e-12]
            if len(prob_density) > 0:
                energy_ent = -np.sum(prob_density * np.log(prob_density))
            else:
                energy_ent = 0.0
        else:
            energy_ent = -np.inf

        # æ–¹æ³•2ï¼šç›´æ¥å½’ä¸€åŒ–
        if magnitude.max() > 0:
            direct_normalized = magnitude / magnitude.max()
            prob = direct_normalized**2 / np.sum(direct_normalized**2)
            prob = prob[prob > 1e-12]
            if len(prob) > 0:
                direct_ent = -np.sum(prob * np.log(prob))
            else:
                direct_ent = 0.0
        else:
            direct_ent = -np.inf

        # æ–¹æ³•3ï¼šä¼ ç»Ÿå›¾åƒç†µ
        if magnitude.max() > 0:
            magnitude_norm = magnitude / magnitude.max()
            magnitude_uint8 = (magnitude_norm * 255).astype(np.uint8)
            hist, _ = np.histogram(magnitude_uint8, bins=256, range=(0, 256), density=True)
            hist = hist[hist > 0]
            if len(hist) > 0:
                traditional_ent = -np.sum(hist * np.log2(hist))
            else:
                traditional_ent = 0.0
        else:
            traditional_ent = -np.inf

        # è¿”å›æ‰€æœ‰æ–¹æ³•çš„ç»“æœï¼Œä¸»è¦ä½¿ç”¨energyå½’ä¸€åŒ–æ–¹æ³•
        return {
            "energy_normalized": energy_ent,
            "direct_normalized": direct_ent,
            "traditional": traditional_ent,
            "energy_factor": g,
        }

    def analyze_sample(self, sample_info, model):
        """åˆ†æå•ä¸ªæ ·æœ¬"""
        sar_path = sample_info["sar"]
        base_name = os.path.basename(sar_path).replace(".raw", "")

        try:
            print(f"  ğŸ” åˆ†ææ ·æœ¬: {base_name}")

            # 1. åŠ è½½åŸå§‹SARå›¾åƒ
            original_tensor = read_sar_complex_tensor(sar_path, config.IMG_HEIGHT, config.IMG_WIDTH)
            if original_tensor is None:
                print(f"    âŒ æ— æ³•åŠ è½½SARå›¾åƒ")
                return None

            original_complex = original_tensor.numpy()

            # 2. æ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                input_tensor = original_tensor.unsqueeze(0).to(config.DEVICE)
                predicted_maps = model(input_tensor).squeeze(0).cpu().numpy()

            # 3. æå–æ•£å°„ä¸­å¿ƒ (Aggressiveæ¨¡å¼)
            predicted_scatterers = self.extract_scatterers_aggressive(predicted_maps)

            # 4. é‡å»ºSARå›¾åƒ
            reconstructed_complex = reconstruct_sar_image(predicted_scatterers)

            # 5. è®¡ç®—MSE
            mse_value = self.calculate_mse_corrected(original_complex, reconstructed_complex)

            # 6. è®¡ç®—ENT (å¤šç§æ–¹æ³•)
            ent_results = self.calculate_entropy_corrected(reconstructed_complex)

            # 7. è·å–GTæ•£å°„ä¸­å¿ƒ
            gt_count = 0
            gt_scatterers = []
            gt_reconstructed = None
            mat_path = os.path.join(project_root, "datasets", "SAR_ASC_Project", "tmp_Training_ASC", f"{base_name}.mat")

            if os.path.exists(mat_path):
                print(f"    ğŸ” è§£æGT: {base_name}.mat")
                gt_scatterers = self.extract_scatterers_from_mat_corrected(mat_path)
                gt_count = len(gt_scatterers)

                # GTé‡å»º
                if gt_scatterers:
                    try:
                        gt_reconstructed = reconstruct_sar_image(gt_scatterers)
                        print(f"    âœ… GTé‡å»ºæˆåŠŸ")
                    except Exception as e:
                        print(f"    âš ï¸ GTé‡å»ºå¤±è´¥: {e}")
            else:
                print(f"    âš ï¸ MATæ–‡ä»¶ä¸å­˜åœ¨")

            # 8. è¾“å‡ºåˆ†æç»“æœ
            print(f"    ğŸ“Š ç»“æœ: MSE={mse_value:.6f}, ENT={ent_results['energy_normalized']:.6f}")
            print(
                f"    ğŸ¯ æ•£å°„ä¸­å¿ƒ: é¢„æµ‹{len(predicted_scatterers)}/GT{gt_count} (æ£€å‡ºç‡:{len(predicted_scatterers)/max(gt_count,1):.2%})"
            )
            print(f"    âš¡ èƒ½é‡å› å­: {ent_results['energy_factor']:.6f}")

            result = {
                "sample_name": base_name,
                "mse": mse_value,
                "entropy_energy": ent_results["energy_normalized"],
                "entropy_direct": ent_results["direct_normalized"],
                "entropy_traditional": ent_results["traditional"],
                "energy_factor": ent_results["energy_factor"],
                "predicted_count": len(predicted_scatterers),
                "gt_count": gt_count,
                "detection_rate": len(predicted_scatterers) / max(gt_count, 1),
                "original_magnitude": np.abs(original_complex),
                "reconstructed_magnitude": np.abs(reconstructed_complex),
                "gt_magnitude": np.abs(gt_reconstructed) if gt_reconstructed is not None else None,
            }

            return result

        except Exception as e:
            print(f"    âŒ åˆ†æé”™è¯¯: {e}")
            import traceback

            traceback.print_exc()
            return None

    def create_comprehensive_visualization(self, results, output_dir):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–"""
        if not results:
            return

        print(f"\nğŸ“Š ç”Ÿæˆç»¼åˆå¯è§†åŒ–åˆ†æ...")

        valid_results = [r for r in results if r is not None]

        # æå–æ•°æ®
        mse_values = [r["mse"] for r in valid_results if np.isfinite(r["mse"])]
        ent_energy = [r["entropy_energy"] for r in valid_results if np.isfinite(r["entropy_energy"])]
        ent_direct = [r["entropy_direct"] for r in valid_results if np.isfinite(r["entropy_direct"])]
        ent_traditional = [r["entropy_traditional"] for r in valid_results if np.isfinite(r["entropy_traditional"])]
        detection_rates = [r["detection_rate"] for r in valid_results]
        energy_factors = [r["energy_factor"] for r in valid_results]

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle("Quantitative Analysis Results (Fixed Version)", fontsize=16)

        # MSEåˆ†å¸ƒ
        if mse_values:
            axes[0, 0].hist(mse_values, bins=20, alpha=0.7, color="blue", edgecolor="black")
            axes[0, 0].set_title(f"MSE Distribution (n={len(mse_values)})")
            axes[0, 0].set_xlabel("Relative RMSE")
            axes[0, 0].set_ylabel("Frequency")
            axes[0, 0].axvline(
                np.mean(mse_values), color="red", linestyle="--", label=f"Mean: {np.mean(mse_values):.3f}"
            )
            axes[0, 0].legend()

        # ç†µå€¼å¯¹æ¯”
        x_pos = np.arange(3)
        ent_means = [
            np.mean(ent_energy) if ent_energy else 0,
            np.mean(ent_direct) if ent_direct else 0,
            np.mean(ent_traditional) if ent_traditional else 0,
        ]
        ent_stds = [
            np.std(ent_energy) if len(ent_energy) > 1 else 0,
            np.std(ent_direct) if len(ent_direct) > 1 else 0,
            np.std(ent_traditional) if len(ent_traditional) > 1 else 0,
        ]

        axes[0, 1].bar(x_pos, ent_means, yerr=ent_stds, capsize=5, alpha=0.7, color=["blue", "green", "orange"])
        axes[0, 1].set_title("Entropy Comparison (Different Methods)")
        axes[0, 1].set_xlabel("Method")
        axes[0, 1].set_ylabel("Entropy Value")
        axes[0, 1].set_xticks(x_pos)
        axes[0, 1].set_xticklabels(["Energy Norm", "Direct Norm", "Traditional"])

        # æ£€å‡ºç‡åˆ†å¸ƒ
        if detection_rates:
            axes[0, 2].hist(detection_rates, bins=20, alpha=0.7, color="green", edgecolor="black")
            axes[0, 2].set_title(f"Detection Rate Distribution")
            axes[0, 2].set_xlabel("Detection Rate")
            axes[0, 2].set_ylabel("Frequency")
            axes[0, 2].axvline(
                np.mean(detection_rates), color="red", linestyle="--", label=f"Mean: {np.mean(detection_rates):.2%}"
            )
            axes[0, 2].legend()

        # MSE vs æ£€å‡ºç‡æ•£ç‚¹å›¾
        if mse_values and detection_rates and len(mse_values) == len(detection_rates):
            axes[1, 0].scatter(detection_rates[: len(mse_values)], mse_values, alpha=0.6)
            axes[1, 0].set_xlabel("Detection Rate")
            axes[1, 0].set_ylabel("MSE (Relative RMSE)")
            axes[1, 0].set_title("MSE vs Detection Rate")

        # èƒ½é‡å› å­åˆ†å¸ƒ
        if energy_factors:
            axes[1, 1].hist(energy_factors, bins=20, alpha=0.7, color="purple", edgecolor="black")
            axes[1, 1].set_title("Energy Factor Distribution")
            axes[1, 1].set_xlabel("Energy Factor")
            axes[1, 1].set_ylabel("Frequency")
            axes[1, 1].set_yscale("log")

        # ç¤ºä¾‹å›¾åƒå¯¹æ¯”
        if valid_results:
            sample_result = valid_results[0]

            # åŸå§‹vsé‡å»ºå¯¹æ¯”
            if sample_result["original_magnitude"] is not None:
                orig_mag = sample_result["original_magnitude"]
                recon_mag = sample_result["reconstructed_magnitude"]

                # å¹¶æ’æ˜¾ç¤º
                combined = np.hstack([orig_mag, recon_mag])
                axes[1, 2].imshow(combined, cmap="gray")
                axes[1, 2].set_title(f'Sample: {sample_result["sample_name"]}\nOriginal | Reconstructed')
                axes[1, 2].axis("off")

                # æ·»åŠ åˆ†å‰²çº¿
                axes[1, 2].axvline(orig_mag.shape[1], color="red", linewidth=2)

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        save_path = os.path.join(output_dir, "quantitative_analysis_fixed_results.png")
        plt.savefig(save_path, dpi=150, bbox_inches="tight")
        plt.close()

        print(f"âœ… å¯è§†åŒ–ä¿å­˜: {save_path}")

    def generate_detailed_report(self, results, output_dir):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        if not results:
            return

        valid_results = [r for r in results if r is not None]

        # ç»Ÿè®¡åˆ†æ
        mse_values = [r["mse"] for r in valid_results if np.isfinite(r["mse"])]
        ent_energy = [r["entropy_energy"] for r in valid_results if np.isfinite(r["entropy_energy"])]
        detection_rates = [r["detection_rate"] for r in valid_results]
        energy_factors = [r["energy_factor"] for r in valid_results]

        # ç”ŸæˆæŠ¥å‘Š
        report_lines = [
            "=" * 80,
            "ğŸ“Š å®šé‡åˆ†æè¯¦ç»†æŠ¥å‘Š (ä¿®å¤ç‰ˆ)",
            "=" * 80,
            "",
            f"ğŸ”§ åˆ†æé…ç½®:",
            f"  æ£€æµ‹æ¨¡å¼: Aggressive",
            f"  ä½é˜ˆå€¼: {self.detection_config['low_threshold']}",
            f"  é«˜é˜ˆå€¼: {self.detection_config['high_threshold']}",
            f"  æœ€å°è·ç¦»: {self.detection_config['min_distance']}",
            "",
            f"ğŸ“ˆ æ•´ä½“ç»Ÿè®¡:",
            f"  æ€»æ ·æœ¬æ•°: {len(valid_results)}",
            f"  æœ‰æ•ˆMSEæ ·æœ¬: {len(mse_values)}",
            f"  æœ‰æ•ˆENTæ ·æœ¬: {len(ent_energy)}",
            "",
        ]

        # MSEç»Ÿè®¡
        if mse_values:
            report_lines.extend(
                [
                    f"ğŸ¯ MSE (ç›¸å¯¹å‡æ–¹æ ¹è¯¯å·®) ç»Ÿè®¡:",
                    f"  å¹³å‡å€¼: {np.mean(mse_values):.6f}",
                    f"  æ ‡å‡†å·®: {np.std(mse_values):.6f}",
                    f"  ä¸­ä½æ•°: {np.median(mse_values):.6f}",
                    f"  æœ€å°å€¼: {np.min(mse_values):.6f}",
                    f"  æœ€å¤§å€¼: {np.max(mse_values):.6f}",
                    f"  25ç™¾åˆ†ä½: {np.percentile(mse_values, 25):.6f}",
                    f"  75ç™¾åˆ†ä½: {np.percentile(mse_values, 75):.6f}",
                    "",
                ]
            )

        # ENTç»Ÿè®¡
        if ent_energy:
            valid_ent = [e for e in ent_energy if e != -np.inf]
            inf_count = len(ent_energy) - len(valid_ent)

            report_lines.extend(
                [
                    f"ğŸŒŠ ENT (Energyå½’ä¸€åŒ–ç†µ) ç»Ÿè®¡:",
                    f"  æœ‰æ•ˆå€¼æ•°é‡: {len(valid_ent)}",
                    f"  æ— æ•ˆå€¼(-inf): {inf_count}",
                ]
            )

            if valid_ent:
                report_lines.extend(
                    [
                        f"  å¹³å‡å€¼: {np.mean(valid_ent):.6f}",
                        f"  æ ‡å‡†å·®: {np.std(valid_ent):.6f}",
                        f"  ä¸­ä½æ•°: {np.median(valid_ent):.6f}",
                        f"  æœ€å°å€¼: {np.min(valid_ent):.6f}",
                        f"  æœ€å¤§å€¼: {np.max(valid_ent):.6f}",
                    ]
                )

            report_lines.append("")

        # æ£€å‡ºç‡ç»Ÿè®¡
        if detection_rates:
            gt_counts = [r["gt_count"] for r in valid_results]
            pred_counts = [r["predicted_count"] for r in valid_results]

            report_lines.extend(
                [
                    f"ğŸ¯ æ•£å°„ä¸­å¿ƒæ£€å‡ºç»Ÿè®¡:",
                    f"  GTå¹³å‡æ•°é‡: {np.mean(gt_counts):.2f}",
                    f"  é¢„æµ‹å¹³å‡æ•°é‡: {np.mean(pred_counts):.2f}",
                    f"  å¹³å‡æ£€å‡ºç‡: {np.mean(detection_rates):.2%}",
                    f"  æ£€å‡ºç‡ä¸­ä½æ•°: {np.median(detection_rates):.2%}",
                    f"  æ£€å‡ºç‡æ ‡å‡†å·®: {np.std(detection_rates):.2%}",
                    "",
                ]
            )

        # æ€§èƒ½åˆ†æ
        if mse_values and ent_energy:
            excellent_mse = len([v for v in mse_values if v < 0.1])
            good_mse = len([v for v in mse_values if v < 0.2])
            valid_ent_count = len([e for e in ent_energy if e > -1])

            report_lines.extend(
                [
                    f"ğŸ“ˆ æ€§èƒ½æ±‡æ€»:",
                    f"  ä¼˜ç§€é‡å»ºæ ·æœ¬ (MSE<0.1): {excellent_mse}/{len(mse_values)} ({excellent_mse/len(mse_values):.1%})",
                    f"  è‰¯å¥½é‡å»ºæ ·æœ¬ (MSE<0.2): {good_mse}/{len(mse_values)} ({good_mse/len(mse_values):.1%})",
                    f"  æœ‰æ•ˆç†µå€¼æ ·æœ¬ (ENT>-1): {valid_ent_count}/{len(ent_energy)} ({valid_ent_count/len(ent_energy):.1%})",
                    "",
                ]
            )

        # èƒ½é‡å› å­åˆ†æ
        if energy_factors:
            zero_energy = len([e for e in energy_factors if e < 1e-10])
            report_lines.extend(
                [
                    f"âš¡ èƒ½é‡å› å­åˆ†æ:",
                    f"  å¹³å‡èƒ½é‡å› å­: {np.mean(energy_factors):.6f}",
                    f"  é›¶èƒ½é‡æ ·æœ¬: {zero_energy}/{len(energy_factors)} ({zero_energy/len(energy_factors):.1%})",
                    "",
                ]
            )

        report_lines.extend(
            [
                "=" * 80,
                "ğŸ’¡ åˆ†æå»ºè®®:",
                "",
                "1. MSEåˆ†æ:",
                "   - å½“å‰MSEå€¼åé«˜ï¼Œè¯´æ˜é‡å»ºç²¾åº¦æœ‰æå‡ç©ºé—´",
                "   - å»ºè®®æ£€æŸ¥æ•£å°„ä¸­å¿ƒå¹…åº¦é¢„æµ‹çš„å‡†ç¡®æ€§",
                "",
                "2. ENTåˆ†æ:",
                "   - å¦‚æœENTå€¼è¿‡å°æˆ–ä¸º-infï¼Œå¯èƒ½æ˜¯é‡å»ºå›¾åƒèƒ½é‡è¿‡ä½",
                "   - å»ºè®®å¢å¼ºæ•£å°„ä¸­å¿ƒå¹…åº¦æˆ–æ”¹è¿›é‡å»ºç®—æ³•",
                "",
                "3. æ£€å‡ºç‡åˆ†æ:",
                "   - Aggressiveæ¨¡å¼ä¸‹çš„æ£€å‡ºç‡åæ˜ äº†æ¨¡å‹çš„æ£€æµ‹èƒ½åŠ›",
                "   - å¯ä»¥è°ƒæ•´é˜ˆå€¼å‚æ•°ä¼˜åŒ–æ£€å‡ºç‡",
                "",
                "=" * 80,
            ]
        )

        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(output_dir, "detailed_analysis_report_fixed.txt")
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))

        print(f"âœ… è¯¦ç»†æŠ¥å‘Šä¿å­˜: {report_path}")


def main():
    print("ğŸ”§ å®šé‡åˆ†æå·¥å…· (ä¿®å¤ç‰ˆ)")
    print("=" * 50)
    print("ä¿®å¤å†…å®¹:")
    print("1. âœ… æ­£ç¡®è§£æMATæ–‡ä»¶çš„åŒé‡åµŒå¥—ç»“æ„")
    print("2. âœ… ä¿®å¤MSEè®¡ç®—æ–¹æ³•")
    print("3. âœ… æ”¹è¿›ENTè®¡ç®—ï¼Œæä¾›å¤šç§æ–¹æ³•å¯¹æ¯”")
    print("4. âœ… å¢å¼ºé”™è¯¯å¤„ç†å’Œè°ƒè¯•è¾“å‡º")
    print("5. âœ… ç”Ÿæˆè¯¦ç»†çš„è¯Šæ–­æŠ¥å‘Š")

    # åŠ è½½æ¨¡å‹
    print("\nğŸ¤– æ­£åœ¨åŠ è½½ASC-Netæ¨¡å‹...")
    model = ASCNet_v3_5param(n_channels=1, n_params=5).to(config.DEVICE)
    model_path = os.path.join(config.CHECKPOINT_DIR, config.MODEL_NAME)

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")

    model.load_state_dict(torch.load(model_path, map_location=config.DEVICE))
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    # åŠ è½½æ•°æ®é›†
    dataset = MSTAR_ASC_5CH_Dataset()
    if not dataset.samples:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬")
        return

    print(f"âœ… æ‰¾åˆ° {len(dataset.samples)} ä¸ªæ ·æœ¬")

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = os.path.join(project_root, "datasets", "result_vis", "quantitative_fixed")
    os.makedirs(output_dir, exist_ok=True)

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = QuantitativeAnalyzerFixed()

    # åˆ†ææ ·æœ¬
    num_samples = min(100, len(dataset.samples))
    print(f"\nğŸ” å¼€å§‹åˆ†æ {num_samples} ä¸ªæ ·æœ¬...")

    results = []
    for i, sample_info in enumerate(tqdm(dataset.samples[:num_samples], desc="åˆ†ææ ·æœ¬")):
        result = analyzer.analyze_sample(sample_info, model)
        if result:
            results.append(result)

    print(f"\nğŸ“Š åˆ†æå®Œæˆï¼æˆåŠŸå¤„ç† {len(results)}/{num_samples} ä¸ªæ ·æœ¬")

    if results:
        # ç”Ÿæˆå¯è§†åŒ–
        analyzer.create_comprehensive_visualization(results, output_dir)

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        analyzer.generate_detailed_report(results, output_dir)

        print(f"\nğŸ‰ ä¿®å¤ç‰ˆå®šé‡åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"\nğŸ” ä¸»è¦å‘ç°:")

        valid_results = [r for r in results if r is not None]
        if valid_results:
            mse_values = [r["mse"] for r in valid_results if np.isfinite(r["mse"])]
            ent_values = [r["entropy_energy"] for r in valid_results if np.isfinite(r["entropy_energy"])]

            if mse_values:
                print(
                    f"  ğŸ“ MSE: å¹³å‡ {np.mean(mse_values):.4f}, èŒƒå›´ [{np.min(mse_values):.4f}, {np.max(mse_values):.4f}]"
                )

            if ent_values:
                valid_ent = [e for e in ent_values if e != -np.inf]
                if valid_ent:
                    print(f"  ğŸŒŠ ENT: æœ‰æ•ˆå€¼ {len(valid_ent)}/{len(ent_values)}, å¹³å‡ {np.mean(valid_ent):.4f}")
                else:
                    print(f"  ğŸŒŠ ENT: å…¨éƒ¨ä¸º-infï¼Œéœ€è¦æ£€æŸ¥é‡å»ºç®—æ³•")
    else:
        print("âŒ æœªèƒ½æˆåŠŸåˆ†æä»»ä½•æ ·æœ¬")


if __name__ == "__main__":
    main()
