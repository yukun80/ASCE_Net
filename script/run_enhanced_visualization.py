# script/run_enhanced_visualization_fixed_layout.py - ä¿®å¤å¸ƒå±€é—®é¢˜ç‰ˆæœ¬

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from PIL import Image
from scipy.ndimage import maximum_filter
import cv2

# --- Setup Project Path ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from model.asc_net import ASCNet_v3_5param
from utils import config
from utils.dataset import MSTAR_ASC_5CH_Dataset, read_sar_complex_tensor
from utils.reconstruction import (
    extract_scatterers_from_mat,
    extract_scatterers_from_prediction_5ch,
    reconstruct_sar_image,
    pixel_to_model,
)

os.environ["KMP_DUPLICATE_LIB_OK"] = "True"

# --- å®šä¹‰åƒç´ é—´è·å¸¸é‡ ---
IMG_HEIGHT = 128
IMG_WIDTH = 128
P_GRID_SIZE = 84
C1 = IMG_HEIGHT / (0.3 * P_GRID_SIZE)
PIXEL_SPACING = 0.1


def find_corresponding_jpg_file_final(sar_path):
    """æŸ¥æ‰¾JPGæ–‡ä»¶"""
    jpg_root = os.path.join(project_root, "datasets", "SAR_ASC_Project", "tmp_Data_Processed_jpg")
    base_name = os.path.basename(sar_path).replace(".raw", "")

    possible_files = [
        os.path.join(jpg_root, f"{base_name}_v2.JPG"),
        os.path.join(jpg_root, f"{base_name}_v2.jpg"),
        os.path.join(jpg_root, f"{base_name}_v1.JPG"),
        os.path.join(jpg_root, f"{base_name}_v1.jpg"),
    ]

    for jpg_path in possible_files:
        if os.path.exists(jpg_path):
            return jpg_path
    return None


def find_corresponding_mat_file_final(sar_path):
    """æŸ¥æ‰¾MATæ–‡ä»¶"""
    mat_root = os.path.join(project_root, "datasets", "SAR_ASC_Project", "tmp_Training_ASC")
    base_name = os.path.basename(sar_path).replace(".raw", "")
    return os.path.join(mat_root, f"{base_name}.mat")


class EnhancedVisualizationProcessor:
    """å¢å¼ºç‰ˆSARå¯è§†åŒ–å¤„ç†å™¨"""

    def __init__(self):
        # æ£€æµ‹é…ç½®
        self.detection_configs = {
            "conservative": {
                "low_threshold": 0.3,
                "high_threshold": 0.6,
                "min_distance": 5,
                "amplitude_weight_low": 0.8,
            },
            "moderate": {"low_threshold": 0.1, "high_threshold": 0.5, "min_distance": 3, "amplitude_weight_low": 0.5},
            "aggressive": {
                "low_threshold": 0.05,
                "high_threshold": 0.3,
                "min_distance": 2,
                "amplitude_weight_low": 0.3,
            },
        }

        # å¢å¼ºé…ç½®
        self.enhancement_configs = {
            "conservative": {"amplitude_boost": 1.5, "weak_boost": 2.0, "contrast_method": "gamma", "gamma": 0.7},
            "moderate": {"amplitude_boost": 2.5, "weak_boost": 3.5, "contrast_method": "adaptive", "gamma": 0.5},
            "aggressive": {"amplitude_boost": 4.0, "weak_boost": 6.0, "contrast_method": "log", "gamma": 0.3},
        }

    def enhanced_extract_scatterers_fixed(self, prediction_maps, config_name="moderate"):
        """ä¿®å¤ç‰ˆæ•£å°„ä¸­å¿ƒæå–å‡½æ•°"""
        config_params = self.detection_configs[config_name]

        heatmap, pred_A_log1p, pred_alpha, pred_dx, pred_dy = [prediction_maps[i] for i in range(5)]
        pred_A = np.expm1(pred_A_log1p)

        # éæœ€å¤§å€¼æŠ‘åˆ¶
        neighborhood_size = 5
        local_max = maximum_filter(heatmap, size=neighborhood_size) == heatmap

        # é«˜é˜ˆå€¼æ£€æµ‹
        high_candidates = (heatmap > config_params["high_threshold"]) & local_max
        high_peak_coords = np.argwhere(high_candidates)

        # ä½é˜ˆå€¼æ£€æµ‹
        low_candidates = (heatmap > config_params["low_threshold"]) & local_max
        low_peak_coords = np.argwhere(low_candidates)

        # åˆå¹¶ç»“æœï¼Œé¿å…é‡å¤
        all_coords = []
        for coord in high_peak_coords:
            all_coords.append((coord[0], coord[1], "high"))

        for coord in low_peak_coords:
            is_duplicate = False
            for existing_coord in high_peak_coords:
                if np.linalg.norm(coord - existing_coord) < config_params["min_distance"]:
                    is_duplicate = True
                    break
            if not is_duplicate:
                all_coords.append((coord[0], coord[1], "low"))

        scatterers = []
        for r, c, strength in all_coords:
            amplitude_weight = 1.0 if strength == "high" else config_params["amplitude_weight_low"]

            x_base, y_base = pixel_to_model(r, c)
            dx = pred_dx[r, c] * PIXEL_SPACING
            dy = pred_dy[r, c] * PIXEL_SPACING
            x_final = x_base + dx
            y_final = y_base - dy

            A_final = pred_A[r, c] * amplitude_weight
            alpha_final = pred_alpha[r, c]

            scatterers.append(
                {
                    "A": A_final,
                    "alpha": alpha_final,
                    "x": x_final,
                    "y": y_final,
                    "gamma": 0,
                    "L": 0,
                    "phi_prime": 0,
                    "strength": strength,
                }
            )

        return scatterers

    def enhanced_reconstruct(self, scatterers, config_name="moderate"):
        """å¢å¼ºé‡å»º"""
        config_params = self.enhancement_configs[config_name]

        boosted_scatterers = []
        for s in scatterers:
            s_copy = s.copy()
            boost = config_params["amplitude_boost"]
            if s.get("strength") == "low":
                boost *= config_params["weak_boost"] / config_params["amplitude_boost"]
            s_copy["A"] = s["A"] * boost
            boosted_scatterers.append(s_copy)

        return reconstruct_sar_image(boosted_scatterers)

    def apply_contrast_enhancement(self, img_complex, config_name="moderate"):
        """åº”ç”¨å¯¹æ¯”åº¦å¢å¼º"""
        config_params = self.enhancement_configs[config_name]
        img_magnitude = np.abs(img_complex)

        if config_params["contrast_method"] == "adaptive":
            img_uint8 = ((img_magnitude / img_magnitude.max()) * 255).astype(np.uint8)
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(img_uint8)
            return enhanced.astype(np.float32) / 255.0 * img_magnitude.max()

        elif config_params["contrast_method"] == "gamma":
            gamma = config_params["gamma"]
            return np.power(img_magnitude / img_magnitude.max(), gamma) * img_magnitude.max()

        elif config_params["contrast_method"] == "log":
            return np.log1p(img_magnitude * 10) / np.log1p(10)

        return img_magnitude


def create_enhanced_comparison_fixed_layout(sample_info, model, processor, output_dir):
    """åˆ›å»ºä¿®å¤å¸ƒå±€é—®é¢˜çš„å¢å¼ºå¯¹æ¯”å¯è§†åŒ–"""
    sar_path = os.path.normpath(sample_info["sar"])
    base_name = os.path.basename(sar_path).replace(".raw", "")

    print(f"\n--- å¤„ç†æ ·æœ¬: {base_name} ---")

    # æŸ¥æ‰¾æ–‡ä»¶
    jpg_path = find_corresponding_jpg_file_final(sar_path)
    mat_path = find_corresponding_mat_file_final(sar_path)

    if not jpg_path or not os.path.exists(mat_path):
        print(f"  âœ— æ–‡ä»¶ç¼ºå¤±")
        return False

    print(f"  âœ“ JPG: {os.path.basename(jpg_path)}")
    print(f"  âœ“ MAT: {os.path.basename(mat_path)}")

    # åŠ è½½æ•°æ®
    try:
        original_sar_image = Image.open(jpg_path)
        sar_tensor = read_sar_complex_tensor(sar_path, config.IMG_HEIGHT, config.IMG_WIDTH)
        gt_scatterers = extract_scatterers_from_mat(mat_path)

        if sar_tensor is None or not gt_scatterers:
            print(f"  âœ— æ•°æ®åŠ è½½å¤±è´¥")
            return False

        print(f"  âœ“ GTæ•£å°„ä¸­å¿ƒ: {len(gt_scatterers)} ä¸ª")

    except Exception as e:
        print(f"  âœ— æ•°æ®åŠ è½½é”™è¯¯: {e}")
        return False

    # GTé‡å»º
    recon_gt = reconstruct_sar_image(gt_scatterers)

    # æ¨¡å‹é¢„æµ‹ - ç¡®ä¿heatmapæ¥è‡ªASC-Netæ¨¡å‹
    print(f"  âœ“ å¼€å§‹ASC-Netæ¨¡å‹æ¨ç†...")
    with torch.no_grad():
        input_tensor = sar_tensor.unsqueeze(0).to(config.DEVICE)
        predicted_maps = model(input_tensor).squeeze(0).cpu().numpy()

    # ç¡®è®¤predicted_maps[0]æ˜¯æ¨¡å‹ç”Ÿæˆçš„heatmap
    model_heatmap = predicted_maps[0]  # è¿™æ˜¯ASC-Netæ¨¡å‹é¢„æµ‹çš„æ•£å°„ä¸­å¿ƒæ¦‚ç‡çƒ­åŠ›å›¾
    print(f"  âœ“ æ¨¡å‹é¢„æµ‹çƒ­åŠ›å›¾èŒƒå›´: [{model_heatmap.min():.3f}, {model_heatmap.max():.3f}]")

    # ä¸åŒé…ç½®çš„ç»“æœ
    configs = ["conservative", "moderate", "aggressive"]  # ç¡®ä¿åŒ…å«æ‰€æœ‰ä¸‰ä¸ªé…ç½®
    results = {}

    for cfg in configs:
        scatterers = processor.enhanced_extract_scatterers_fixed(predicted_maps, cfg)
        recon = processor.enhanced_reconstruct(scatterers, cfg)
        enhanced = processor.apply_contrast_enhancement(recon, cfg)

        results[cfg] = {
            "scatterers": scatterers,
            "reconstruction": recon,
            "enhanced": enhanced,
            "count": len(scatterers),
        }
        print(f"  âœ“ {cfg.title()}: {len(scatterers)} ä¸ªæ•£å°„ä¸­å¿ƒ")

    # ä¿®å¤å¸ƒå±€é—®é¢˜ï¼šä½¿ç”¨æ›´åˆç†çš„subplotå¸ƒå±€
    fig = plt.figure(figsize=(24, 12))
    fig.suptitle(f"Enhanced SAR Reconstruction: {base_name}", fontsize=18, y=0.95)

    # åˆ›å»ºç½‘æ ¼å¸ƒå±€ï¼š3è¡Œ5åˆ—
    gs = fig.add_gridspec(
        3, 5, height_ratios=[1, 1, 0.3], width_ratios=[1, 1, 1, 1, 1], hspace=0.25, wspace=0.15, top=0.9, bottom=0.25
    )

    # ç¬¬ä¸€è¡Œï¼šåŸå§‹å›¾åƒ + GT + ä¸‰ç§é…ç½®çš„é‡å»º
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.imshow(original_sar_image, cmap="gray")
    ax1.set_title("Original SAR\n(v2 JPG)", fontsize=12)
    ax1.axis("off")

    ax2 = fig.add_subplot(gs[0, 1])
    gt_vmax = np.percentile(np.abs(recon_gt), 99.5)
    ax2.imshow(np.abs(recon_gt), cmap="gray", vmin=0, vmax=gt_vmax)
    ax2.set_title(f"GT Reconstruction\n({len(gt_scatterers)} scatterers)", fontsize=12)
    ax2.axis("off")

    # ç¡®ä¿æ˜¾ç¤ºæ‰€æœ‰ä¸‰ä¸ªé…ç½®
    for i, cfg in enumerate(configs):
        ax = fig.add_subplot(gs[0, i + 2])
        vmax = np.percentile(np.abs(results[cfg]["reconstruction"]), 99.5)
        ax.imshow(np.abs(results[cfg]["reconstruction"]), cmap="gray", vmin=0, vmax=vmax)
        ax.set_title(f"{cfg.title()}\n({results[cfg]['count']} scatterers)", fontsize=12)
        ax.axis("off")

    # ç¬¬äºŒè¡Œï¼šæ¨¡å‹é¢„æµ‹çƒ­åŠ›å›¾ + ä¸‰ç§é…ç½®çš„å¢å¼ºå›¾åƒ
    ax_heatmap = fig.add_subplot(gs[1, 0])
    im_heatmap = ax_heatmap.imshow(model_heatmap, cmap="hot", interpolation="bilinear")
    ax_heatmap.set_title("ASC-Net Prediction\nHeatmap", fontsize=12)
    ax_heatmap.axis("off")
    plt.colorbar(im_heatmap, ax=ax_heatmap, fraction=0.046, pad=0.04)

    # æ˜¾ç¤ºGTé‡å»ºçš„å¢å¼ºç‰ˆæœ¬
    ax_gt_enhanced = fig.add_subplot(gs[1, 1])
    # å¯¹GTé‡å»ºåº”ç”¨adaptiveå¢å¼º
    gt_magnitude = np.abs(recon_gt)
    gt_uint8 = ((gt_magnitude / gt_magnitude.max()) * 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    gt_enhanced = clahe.apply(gt_uint8)
    gt_enhanced = gt_enhanced.astype(np.float32) / 255.0 * gt_magnitude.max()

    ax_gt_enhanced.imshow(gt_enhanced, cmap="gray")
    ax_gt_enhanced.set_title("Enhanced GT", fontsize=12)
    ax_gt_enhanced.axis("off")

    # æ˜¾ç¤ºä¸‰ç§é…ç½®çš„å¢å¼ºå›¾åƒ
    for i, cfg in enumerate(configs):
        ax = fig.add_subplot(gs[1, i + 2])
        ax.imshow(results[cfg]["enhanced"], cmap="gray")
        ax.set_title(f"Enhanced {cfg.title()}", fontsize=12)
        ax.axis("off")

    # ç¬¬ä¸‰è¡Œï¼šé…ç½®å‚æ•°è¡¨æ ¼ï¼ˆä¸é‡å ï¼‰
    ax_config = fig.add_subplot(gs[2, :])
    ax_config.axis("off")

    # åˆ›å»ºé…ç½®å‚æ•°è¡¨æ ¼
    config_data = []
    headers = [
        "Configuration",
        "Low Threshold",
        "High Threshold",
        "Amplitude Boost",
        "Weak Boost",
        "Method",
        "Detected",
    ]

    for cfg in configs:
        det_cfg = processor.detection_configs[cfg]
        enh_cfg = processor.enhancement_configs[cfg]
        config_data.append(
            [
                cfg.title(),
                f"{det_cfg['low_threshold']:.2f}",
                f"{det_cfg['high_threshold']:.2f}",
                f"{enh_cfg['amplitude_boost']:.1f}x",
                f"{enh_cfg['weak_boost']:.1f}x",
                enh_cfg["contrast_method"],
                f"{results[cfg]['count']}",
            ]
        )

    # åˆ›å»ºè¡¨æ ¼
    table = ax_config.table(
        cellText=config_data, colLabels=headers, cellLoc="center", loc="center", bbox=[0.1, 0.1, 0.8, 0.8]
    )
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 2)

    # è®¾ç½®è¡¨æ ¼æ ·å¼
    for i in range(len(headers)):
        table[(0, i)].set_facecolor("#40466e")
        table[(0, i)].set_text_props(weight="bold", color="white")

    for i in range(1, len(config_data) + 1):
        for j in range(len(headers)):
            if i % 2 == 0:
                table[(i, j)].set_facecolor("#f1f1f2")

    # ä¿å­˜ç»“æœ
    save_path = os.path.join(output_dir, f"enhanced_comparison_{base_name}.png")
    plt.savefig(save_path, dpi=150, bbox_inches="tight")
    plt.close(fig)

    print(f"  âœ“ ä¿å­˜æˆåŠŸ: {os.path.basename(save_path)}")

    # è¾“å‡ºè¯¦ç»†å¯¹æ¯”
    print(f"  ğŸ“Š ASC-Netæ¨¡å‹æ•ˆæœå¯¹æ¯”:")
    for cfg in configs:
        improvement = results[cfg]["count"] / len(gt_scatterers) * 100
        print(f"     {cfg.title()}: {results[cfg]['count']}/{len(gt_scatterers)} " f"({improvement:.1f}% æ£€å‡ºç‡)")

    return True


def main():
    print("=== SARé‡å»ºå¯è§†åŒ–å¢å¼ºå·¥å…· (ä¿®å¤å¸ƒå±€ç‰ˆ) ===")
    print("ä¿®å¤å†…å®¹:")
    print("1. è§£å†³æ–‡æœ¬é‡å é—®é¢˜")
    print("2. ç¡®ä¿æ˜¾ç¤ºæ‰€æœ‰ä¸‰ä¸ªé…ç½®")
    print("3. ç¡®è®¤heatmapæ¥è‡ªASC-Netæ¨¡å‹")

    # åŠ è½½æ¨¡å‹
    print("\næ­£åœ¨åŠ è½½ASC-Netæ¨¡å‹...")
    model = ASCNet_v3_5param(n_channels=1, n_params=5).to(config.DEVICE)
    model_path = os.path.join(config.CHECKPOINT_DIR, config.MODEL_NAME)

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")

    model.load_state_dict(torch.load(model_path, map_location=config.DEVICE))
    model.eval()
    print("âœ“ ASC-Netæ¨¡å‹åŠ è½½æˆåŠŸ")

    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = EnhancedVisualizationProcessor()

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = os.path.join(project_root, "datasets", "result_vis", "comprehensive_results")
    os.makedirs(output_dir, exist_ok=True)
    print(f"âœ“ ç»“æœä¿å­˜ç›®å½•: {output_dir}")

    # åŠ è½½æ•°æ®é›†
    dataset = MSTAR_ASC_5CH_Dataset()
    if not dataset.samples:
        print("âœ— æœªæ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬")
        return

    print(f"âœ“ æ‰¾åˆ° {len(dataset.samples)} ä¸ªæ ·æœ¬")

    # å¤„ç†æ ·æœ¬
    num_samples = min(2000, len(dataset.samples))
    successful_samples = 0

    print(f"\n=== å¼€å§‹å¤„ç† {num_samples} ä¸ªæ ·æœ¬ ===")

    for i, sample_info in enumerate(dataset.samples[:num_samples]):
        try:
            if create_enhanced_comparison_fixed_layout(sample_info, model, processor, output_dir):
                successful_samples += 1
        except Exception as e:
            print(f"  âœ— å¤„ç†é”™è¯¯: {e}")
            import traceback

            traceback.print_exc()
            continue

    # æ€»ç»“
    print(f"\n=== å¤„ç†å®Œæˆ ===")
    print(f"æˆåŠŸå¤„ç†: {successful_samples}/{num_samples} ä¸ªæ ·æœ¬")

    if successful_samples > 0:
        print(f"\nğŸ‰ æˆåŠŸç”Ÿæˆä¿®å¤å¸ƒå±€çš„å¯¹æ¯”å›¾ï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"\nâœ… ä¿®å¤å†…å®¹ç¡®è®¤:")
        print("  â€¢ å¸ƒå±€é—®é¢˜ï¼šä½¿ç”¨ç½‘æ ¼å¸ƒå±€ï¼Œé¿å…æ–‡æœ¬é‡å ")
        print("  â€¢ æ˜¾ç¤ºå®Œæ•´ï¼šåŒ…å«Conservativeã€Moderateã€Aggressiveä¸‰ç§é…ç½®")
        print("  â€¢ Heatmapæ¥æºï¼šç¡®è®¤æ¥è‡ªASC-Netæ¨¡å‹é¢„æµ‹")
        print("  â€¢ å‚æ•°è¡¨æ ¼ï¼šæ¸…æ™°æ˜¾ç¤ºå„é…ç½®å‚æ•°")

        print(f"\nğŸ“Š å¯è§†åŒ–è¯´æ˜:")
        print("  â€¢ ç¬¬ä¸€è¡Œï¼šåŸå§‹å›¾åƒã€GTé‡å»ºã€ä¸‰ç§é…ç½®é‡å»º")
        print("  â€¢ ç¬¬äºŒè¡Œï¼šASC-Neté¢„æµ‹çƒ­åŠ›å›¾ã€ä¸‰ç§é…ç½®å¢å¼ºå›¾åƒ")
        print("  â€¢ ç¬¬ä¸‰è¡Œï¼šè¯¦ç»†å‚æ•°é…ç½®è¡¨æ ¼")
    else:
        print("\nâš ï¸ æœªèƒ½æˆåŠŸå¤„ç†ä»»ä½•æ ·æœ¬")


if __name__ == "__main__":
    main()
