#!/usr/bin/env python3
# script/visualize_scatterer_overlay.py - æ•£å°„ä¸­å¿ƒå åŠ å¯è§†åŒ–å·¥å…·

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import Circle, Rectangle
from tqdm import tqdm
from PIL import Image
from scipy.ndimage import maximum_filter
import cv2

# --- Setup Project Path ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from model.asc_net import ASCNet_v3_5param
from utils import config
from utils.dataset import MSTAR_ASC_5CH_Dataset, read_sar_complex_tensor
from utils.reconstruction import (
    extract_scatterers_from_mat,
    extract_scatterers_from_prediction_5ch,
    pixel_to_model,
)

os.environ["KMP_DUPLICATE_LIB_OK"] = "True"

# --- å®šä¹‰åƒç´ é—´è·å¸¸é‡ ---
IMG_HEIGHT = 128
IMG_WIDTH = 128
P_GRID_SIZE = 84
PIXEL_SPACING = 0.1


def model_to_pixel(x_model, y_model):
    """
    å°†ç‰©ç†æ¨¡å‹åæ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡ï¼ˆpixel_to_modelçš„é€†å‡½æ•°ï¼‰

    Parameters:
    -----------
    x_model, y_model : float
        ç‰©ç†åæ ‡ï¼ˆç±³ï¼‰

    Returns:
    --------
    row, col : float
        åƒç´ åæ ‡
    """
    C1 = IMG_HEIGHT / (0.3 * P_GRID_SIZE)
    C2 = IMG_WIDTH / 2  # 64.0

    # é€†å˜æ¢
    row = C2 - y_model * C1
    col = C2 + x_model * C1

    return row, col


def find_corresponding_jpg_file(sar_path):
    """æŸ¥æ‰¾å¯¹åº”çš„JPGæ–‡ä»¶"""
    jpg_root = os.path.join(project_root, "datasets", "SAR_ASC_Project", "tmp_Data_Processed_jpg")
    base_name = os.path.basename(sar_path).replace(".raw", "")

    possible_files = [
        os.path.join(jpg_root, f"{base_name}_v2.JPG"),
        os.path.join(jpg_root, f"{base_name}_v2.jpg"),
        os.path.join(jpg_root, f"{base_name}_v1.JPG"),
        os.path.join(jpg_root, f"{base_name}_v1.jpg"),
    ]

    for jpg_path in possible_files:
        if os.path.exists(jpg_path):
            return jpg_path
    return None


def find_corresponding_mat_file(sar_path):
    """æŸ¥æ‰¾å¯¹åº”çš„MATæ–‡ä»¶"""
    mat_root = os.path.join(project_root, "datasets", "SAR_ASC_Project", "tmp_Training_ASC")
    base_name = os.path.basename(sar_path).replace(".raw", "")
    return os.path.join(mat_root, f"{base_name}.mat")


class ScattererVisualizationProcessor:
    """æ•£å°„ä¸­å¿ƒå åŠ å¯è§†åŒ–å¤„ç†å™¨"""

    def __init__(self):
        # æ£€æµ‹é…ç½®
        self.detection_configs = {
            "conservative": {
                "low_threshold": 0.3,
                "high_threshold": 0.6,
                "min_distance": 5,
                "amplitude_weight_low": 0.8,
            },
            "moderate": {
                "low_threshold": 0.1,
                "high_threshold": 0.5,
                "min_distance": 3,
                "amplitude_weight_low": 0.5,
            },
            "aggressive": {
                "low_threshold": 0.05,
                "high_threshold": 0.3,
                "min_distance": 2,
                "amplitude_weight_low": 0.3,
            },
        }

        # å¯è§†åŒ–æ ·å¼é…ç½® - ç»Ÿä¸€ä½¿ç”¨çº¢ç‚¹
        self.marker_styles = {
            "gt": {
                "color": "red",
                "marker": "o",
                "size": 60,
                "alpha": 0.8,
                "edgecolor": "darkred",
                "linewidth": 1.5,
                "label": "Ground Truth",
            },
            "pred_high": {
                "color": "red",
                "marker": "o",
                "size": 60,
                "alpha": 0.8,
                "edgecolor": "darkred",
                "linewidth": 1.5,
                "label": "High Confidence",
            },
            "pred_low": {
                "color": "red",
                "marker": "o",
                "size": 60,
                "alpha": 0.8,
                "edgecolor": "darkred",
                "linewidth": 1.5,
                "label": "Low Confidence",
            },
        }

    def enhanced_extract_scatterers(self, prediction_maps, config_name="moderate"):
        """å¢å¼ºç‰ˆæ•£å°„ä¸­å¿ƒæå–"""
        config_params = self.detection_configs[config_name]

        heatmap, pred_A_log1p, pred_alpha, pred_dx, pred_dy = [prediction_maps[i] for i in range(5)]
        pred_A = np.expm1(pred_A_log1p)

        # éæœ€å¤§å€¼æŠ‘åˆ¶
        neighborhood_size = 5
        local_max = maximum_filter(heatmap, size=neighborhood_size) == heatmap

        # é«˜é˜ˆå€¼æ£€æµ‹
        high_candidates = (heatmap > config_params["high_threshold"]) & local_max
        high_peak_coords = np.argwhere(high_candidates)

        # ä½é˜ˆå€¼æ£€æµ‹
        low_candidates = (heatmap > config_params["low_threshold"]) & local_max
        low_peak_coords = np.argwhere(low_candidates)

        # åˆå¹¶ç»“æœï¼Œé¿å…é‡å¤
        all_coords = []
        for coord in high_peak_coords:
            all_coords.append((coord[0], coord[1], "high"))

        for coord in low_peak_coords:
            is_duplicate = False
            for existing_coord in high_peak_coords:
                if np.linalg.norm(coord - existing_coord) < config_params["min_distance"]:
                    is_duplicate = True
                    break
            if not is_duplicate:
                all_coords.append((coord[0], coord[1], "low"))

        scatterers = []
        for r, c, strength in all_coords:
            amplitude_weight = 1.0 if strength == "high" else config_params["amplitude_weight_low"]

            x_base, y_base = pixel_to_model(r, c)
            dx = pred_dx[r, c] * PIXEL_SPACING
            dy = pred_dy[r, c] * PIXEL_SPACING
            x_final = x_base + dx
            y_final = y_base - dy

            A_final = pred_A[r, c] * amplitude_weight
            alpha_final = pred_alpha[r, c]

            scatterers.append(
                {
                    "A": A_final,
                    "alpha": alpha_final,
                    "x": x_final,
                    "y": y_final,
                    "gamma": 0,
                    "L": 0,
                    "phi_prime": 0,
                    "strength": strength,
                    "pixel_row": r,
                    "pixel_col": c,
                    "confidence": heatmap[r, c],
                }
            )

        return scatterers

    def create_overlay_visualization(
        self, sar_image, gt_scatterers, pred_scatterers, config_name="moderate", show_heatmap=True, prediction_maps=None
    ):
        """åˆ›å»ºæ•£å°„ä¸­å¿ƒå åŠ å¯è§†åŒ–"""

        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle(f"Scatterer Detection Overlay Visualization ({config_name.title()} Config)", fontsize=16, y=0.95)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ä¾¿å¤„ç† - ç¡®ä¿æ­£ç¡®çš„ç°åº¦æ˜¾ç¤º
        if isinstance(sar_image, Image.Image):
            sar_array = np.array(sar_image)  # ä¿æŒåŸå§‹æ•°ç»„æ ¼å¼
        else:
            sar_array = sar_image

        # 1. åŸå§‹SARå›¾åƒ + GTæ•£å°„ä¸­å¿ƒ
        ax1 = axes[0]
        ax1.imshow(sar_array, cmap="gray", alpha=0.8)  # ä½¿ç”¨ç°åº¦colormapç¡®ä¿æ­£ç¡®æ˜¾ç¤º

        # ç»˜åˆ¶GTæ•£å°„ä¸­å¿ƒ
        for scatterer in gt_scatterers:
            row, col = model_to_pixel(scatterer["x"], scatterer["y"])
            if 0 <= row < IMG_HEIGHT and 0 <= col < IMG_WIDTH:
                style = self.marker_styles["gt"]
                ax1.scatter(
                    col,
                    row,
                    c=style["color"],
                    marker=style["marker"],
                    s=style["size"],
                    alpha=style["alpha"],
                    edgecolors=style["edgecolor"],
                    linewidths=style["linewidth"],
                )

        ax1.set_title(f"Ground Truth Scatterers\n({len(gt_scatterers)} detected)", fontsize=12)
        ax1.axis("off")

        # 2. åŸå§‹SARå›¾åƒ + é¢„æµ‹æ•£å°„ä¸­å¿ƒ
        ax2 = axes[1]
        ax2.imshow(sar_array, cmap="gray", alpha=0.8)  # ä½¿ç”¨ç°åº¦colormapç¡®ä¿æ­£ç¡®æ˜¾ç¤º

        # åˆ†åˆ«ç»˜åˆ¶é«˜ä½ç½®ä¿¡åº¦æ•£å°„ä¸­å¿ƒ
        high_conf = [s for s in pred_scatterers if s["strength"] == "high"]
        low_conf = [s for s in pred_scatterers if s["strength"] == "low"]

        for scatterer in high_conf:
            row, col = model_to_pixel(scatterer["x"], scatterer["y"])
            if 0 <= row < IMG_HEIGHT and 0 <= col < IMG_WIDTH:
                style = self.marker_styles["pred_high"]
                ax2.scatter(
                    col,
                    row,
                    c=style["color"],
                    marker=style["marker"],
                    s=style["size"],
                    alpha=style["alpha"],
                    edgecolors=style["edgecolor"],
                    linewidths=style["linewidth"],
                )

        for scatterer in low_conf:
            row, col = model_to_pixel(scatterer["x"], scatterer["y"])
            if 0 <= row < IMG_HEIGHT and 0 <= col < IMG_WIDTH:
                style = self.marker_styles["pred_low"]
                ax2.scatter(
                    col,
                    row,
                    c=style["color"],
                    marker=style["marker"],
                    s=style["size"],
                    alpha=style["alpha"],
                    edgecolors=style["edgecolor"],
                    linewidths=style["linewidth"],
                )

        ax2.set_title(f"Predicted Scatterers\n({len(pred_scatterers)} detected)", fontsize=12)
        ax2.axis("off")

        # æ·»åŠ å›¾ä¾‹ - ç®€åŒ–ä¸ºç»Ÿä¸€çš„çº¢ç‚¹æ ‡è®°
        legend_elements = [
            plt.scatter(
                [],
                [],
                c="red",
                marker="o",
                s=60,
                alpha=0.8,
                edgecolors="darkred",
                linewidths=1.5,
                label="Detected Scatterers",
            )
        ]

        fig.legend(handles=legend_elements, loc="lower center", bbox_to_anchor=(0.5, 0.02), ncol=1, fontsize=12)

        plt.tight_layout()
        plt.subplots_adjust(bottom=0.15)

        return fig

    def calculate_detection_metrics(self, gt_scatterers, pred_scatterers, distance_threshold=0.5):
        """è®¡ç®—æ£€æµ‹æ€§èƒ½æŒ‡æ ‡"""
        # é…å¯¹ç®—æ³•ï¼šæ‰¾åˆ°è·ç¦»æœ€è¿‘çš„GTå’Œé¢„æµ‹æ•£å°„ä¸­å¿ƒ
        matched_pairs = []
        unmatched_gt = list(range(len(gt_scatterers)))
        unmatched_pred = list(range(len(pred_scatterers)))

        for i, gt_sc in enumerate(gt_scatterers):
            best_match_idx = None
            min_distance = float("inf")

            for j, pred_sc in enumerate(pred_scatterers):
                if j in unmatched_pred:
                    distance = np.sqrt((gt_sc["x"] - pred_sc["x"]) ** 2 + (gt_sc["y"] - pred_sc["y"]) ** 2)
                    if distance < min_distance and distance < distance_threshold:
                        min_distance = distance
                        best_match_idx = j

            if best_match_idx is not None:
                matched_pairs.append((i, best_match_idx, min_distance))
                unmatched_gt.remove(i)
                unmatched_pred.remove(best_match_idx)

        # è®¡ç®—æŒ‡æ ‡
        true_positives = len(matched_pairs)
        false_positives = len(unmatched_pred)
        false_negatives = len(unmatched_gt)

        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        return {
            "true_positives": true_positives,
            "false_positives": false_positives,
            "false_negatives": false_negatives,
            "precision": precision,
            "recall": recall,
            "f1_score": f1_score,
            "matched_pairs": matched_pairs,
            "avg_distance": np.mean([pair[2] for pair in matched_pairs]) if matched_pairs else 0,
        }


def create_scatterer_overlay_visualization(sample_info, model, processor, output_dir, config_name="moderate"):
    """ä¸ºå•ä¸ªæ ·æœ¬åˆ›å»ºæ•£å°„ä¸­å¿ƒå åŠ å¯è§†åŒ–"""
    sar_path = os.path.normpath(sample_info["sar"])
    base_name = os.path.basename(sar_path).replace(".raw", "")

    print(f"\n--- å¤„ç†æ ·æœ¬: {base_name} ---")

    # æŸ¥æ‰¾æ–‡ä»¶
    jpg_path = find_corresponding_jpg_file(sar_path)
    mat_path = find_corresponding_mat_file(sar_path)

    if not jpg_path or not os.path.exists(mat_path):
        print(f"  âœ— æ–‡ä»¶ç¼ºå¤±")
        return False, None

    print(f"  âœ“ JPG: {os.path.basename(jpg_path)}")
    print(f"  âœ“ MAT: {os.path.basename(mat_path)}")

    # åŠ è½½æ•°æ®
    try:
        original_sar_image = Image.open(jpg_path)
        sar_tensor = read_sar_complex_tensor(sar_path, config.IMG_HEIGHT, config.IMG_WIDTH)
        gt_scatterers = extract_scatterers_from_mat(mat_path)

        if sar_tensor is None or not gt_scatterers:
            print(f"  âœ— æ•°æ®åŠ è½½å¤±è´¥")
            return False, None

        print(f"  âœ“ GTæ•£å°„ä¸­å¿ƒ: {len(gt_scatterers)} ä¸ª")

    except Exception as e:
        print(f"  âœ— æ•°æ®åŠ è½½é”™è¯¯: {e}")
        return False, None

    # æ¨¡å‹é¢„æµ‹
    print(f"  âœ“ å¼€å§‹ASC-Netæ¨¡å‹æ¨ç†...")
    with torch.no_grad():
        input_tensor = sar_tensor.unsqueeze(0).to(config.DEVICE)
        predicted_maps = model(input_tensor).squeeze(0).cpu().numpy()

    # æå–é¢„æµ‹æ•£å°„ä¸­å¿ƒ
    pred_scatterers = processor.enhanced_extract_scatterers(predicted_maps, config_name)
    print(f"  âœ“ é¢„æµ‹æ•£å°„ä¸­å¿ƒ: {len(pred_scatterers)} ä¸ª")

    # è®¡ç®—æ£€æµ‹æŒ‡æ ‡
    metrics = processor.calculate_detection_metrics(gt_scatterers, pred_scatterers)
    print(f"  ğŸ“Š æ£€æµ‹æ€§èƒ½: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}")

    # åˆ›å»ºå¯è§†åŒ–
    fig = processor.create_overlay_visualization(
        original_sar_image,
        gt_scatterers,
        pred_scatterers,
        config_name,
        show_heatmap=True,
        prediction_maps=predicted_maps,
    )

    # ä¿å­˜ç»“æœ
    save_path = os.path.join(output_dir, f"scatterer_overlay_{config_name}_{base_name}.png")
    fig.savefig(save_path, dpi=150, bbox_inches="tight")
    plt.close(fig)

    print(f"  âœ“ ä¿å­˜æˆåŠŸ: {os.path.basename(save_path)}")

    return True, metrics


def main():
    print("=== æ•£å°„ä¸­å¿ƒå åŠ å¯è§†åŒ–å·¥å…· (ç®€åŒ–ç‰ˆ) ===")
    print("åŠŸèƒ½:")
    print("1. åœ¨åŸå§‹SARå›¾åƒä¸Šå åŠ GTå’Œé¢„æµ‹æ•£å°„ä¸­å¿ƒ")
    print("2. æä¾›ç›´è§‚çš„æ£€æµ‹æ•ˆæœå¯¹æ¯”")
    print("3. è®¡ç®—æ£€æµ‹æ€§èƒ½æŒ‡æ ‡")
    print("4. ç®€åŒ–ç‰ˆ: åªæ˜¾ç¤ºGTå’Œé¢„æµ‹ä¸¤ä¸ªå­å›¾")

    # åŠ è½½æ¨¡å‹
    print("\næ­£åœ¨åŠ è½½ASC-Netæ¨¡å‹...")
    model = ASCNet_v3_5param(n_channels=1, n_params=5).to(config.DEVICE)
    model_path = os.path.join(config.CHECKPOINT_DIR, config.MODEL_NAME)

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")

    model.load_state_dict(torch.load(model_path, map_location=config.DEVICE))
    model.eval()
    print("âœ“ ASC-Netæ¨¡å‹åŠ è½½æˆåŠŸ")

    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = ScattererVisualizationProcessor()

    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = os.path.join(project_root, "datasets", "result_vis", "scatterer_overlay_results")
    os.makedirs(output_dir, exist_ok=True)
    print(f"âœ“ ç»“æœä¿å­˜ç›®å½•: {output_dir}")

    # åŠ è½½æ•°æ®é›†
    dataset = MSTAR_ASC_5CH_Dataset()
    if not dataset.samples:
        print("âœ— æœªæ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬")
        return

    print(f"âœ“ æ‰¾åˆ° {len(dataset.samples)} ä¸ªæ ·æœ¬")

    # é€‰æ‹©å¤„ç†é…ç½®
    configs_to_test = ["conservative", "moderate", "aggressive"]

    # å¤„ç†æ ·æœ¬
    num_samples = min(500, len(dataset.samples))  # å¤„ç†å‰50ä¸ªæ ·æœ¬

    print(f"\n=== å¼€å§‹å¤„ç† {num_samples} ä¸ªæ ·æœ¬ï¼Œ{len(configs_to_test)} ç§é…ç½® ===")

    all_metrics = {config: [] for config in configs_to_test}
    successful_samples = 0

    for i, sample_info in enumerate(tqdm(dataset.samples[:num_samples], desc="å¤„ç†æ ·æœ¬")):
        sample_success = False

        for config_name in configs_to_test:
            try:
                success, metrics = create_scatterer_overlay_visualization(
                    sample_info, model, processor, output_dir, config_name
                )
                if success:
                    all_metrics[config_name].append(metrics)
                    sample_success = True

            except Exception as e:
                print(f"  âœ— {config_name}é…ç½®å¤„ç†é”™è¯¯: {e}")
                continue

        if sample_success:
            successful_samples += 1

    # è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
    print(f"\n=== æ€§èƒ½ç»Ÿè®¡ ===")
    print(f"æˆåŠŸå¤„ç†: {successful_samples}/{num_samples} ä¸ªæ ·æœ¬")

    for config_name in configs_to_test:
        metrics_list = all_metrics[config_name]
        if metrics_list:
            avg_precision = np.mean([m["precision"] for m in metrics_list])
            avg_recall = np.mean([m["recall"] for m in metrics_list])
            avg_f1 = np.mean([m["f1_score"] for m in metrics_list])
            avg_distance = np.mean([m["avg_distance"] for m in metrics_list])

            print(f"\n{config_name.title()} é…ç½®å¹³å‡æ€§èƒ½:")
            print(f"  Precision: {avg_precision:.3f}")
            print(f"  Recall: {avg_recall:.3f}")
            print(f"  F1-Score: {avg_f1:.3f}")
            print(f"  Avg Match Distance: {avg_distance:.3f}m")

    if successful_samples > 0:
        print(f"\nğŸ‰ æ•£å°„ä¸­å¿ƒå åŠ å¯è§†åŒ–å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"\nâœ… å¯è§†åŒ–å†…å®¹:")
        print("  â€¢ å·¦ï¼šåŸå§‹SAR + GTæ•£å°„ä¸­å¿ƒï¼ˆçº¢è‰²åœ†ç‚¹ï¼‰")
        print("  â€¢ å³ï¼šåŸå§‹SAR + é¢„æµ‹æ•£å°„ä¸­å¿ƒï¼ˆçº¢è‰²åœ†ç‚¹ï¼‰")
        print(f"\nğŸ“Š ç”Ÿæˆäº† {len(configs_to_test)} ç§æ£€æµ‹é…ç½®çš„å¯¹æ¯”ç»“æœ")
    else:
        print("\nâš ï¸ æœªèƒ½æˆåŠŸå¤„ç†ä»»ä½•æ ·æœ¬")


if __name__ == "__main__":
    main()
